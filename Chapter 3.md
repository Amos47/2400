

---
Here's a quiz about Gitbook

|                  | Good | Bad |
| ---------------- | ---- | --- |
| What is Gitbook? | (x)  | ( ) |

> Gitbook is good

What does Gitbook support?
- [x] Table-based questions with radio buttons
- [x] Table-based questions with checkboxes
- [ ] Telepathy
- [x] List-based questions with checkboxes
- [x] List-based questions with radio buttons
- [ ] Moon-on-a-stick

> Gitbook supports table and list based quiz questions using either radio buttons or checkboxes.
>
> Gitbook is not telepathic and does not give you the moon on a stick.

---


In this chapter we will consider the broad topic of how systems come to be. What sparks the need for a system or a system change? How are system requirements decided?  How are systems built? How are they put into place in an organisation? How are they cared for during their life-span and how are they retired?

This chapter is about more than building new systems. It is also about NOT building systems when a new system is required, rather acquiring it through other channels. And it's about how organisations decide whether to buy, build, rent or commission the building of a brand-new system.

## What sparks the need for a new system in a typical organisation?

The on-line [Merriam-Webster Dictionary](http://www.merriam-webster.com/dictionary/process) defines *process* simply as "a series of actions that produce something or that lead to a particular result." Recall our discussion of a simple process in Chapter 1. *Input* **-->** *Process* **-->** *Output*. Simple and clean. When you stop to think about it, everything that happens in any organisation (and in your life) is part of a process. Something spurs action (you feel cold in your room - this is input) so you either turn up the thermostat, or put on a sweater or close an open window (this is the process part). From this process action,  some output is produced (something is changed - more warmth or less cold but in the end, you feel more comfortable).

It's the same in an organisation. Something happens as input (profits are falling; employees are calling in sick in record numbers; a competitor releases a new product or it's simply time for a strategic review - the ticking of the clock has led to a milestone being hit, triggering a review). This input bubbles through the organisation until it's recognised by someone or some monitoring process, causing someone or some group to start mounting a response.

One of those possible *somethings that could be done* is a change to an existing system or the recognition that an entirely new system is required. This decision would be made using one of the decision models discussed in Chapter 3, where we also discuss strategic alignment. For now, let's just assume that a decision was made to respond to the input stimulus through the tactic of introducing (what is often called *sourcing*) a new (meaning new *to the organisation*) system.

## Sourcing a new system
What are the choices when sourcing a new system? There are several different variations, leading to eleven distinct possible paths plus (just to make a long story even longer) several possibilities for combing one or more of the eleven. 

We will first make the distinction of whether the system will be: A) custom build using some combination of existing components and/or brand new code, or; B) if the system is sourced from the marketplace of existing software systems that any organisation could purchase or rent on the open market. Purchased commercial software is referred to as COTS (for *Commercial Off-The-Shelf*) software (such as Microsoft Excel), while rented software is referred to as *Software as a Service* or SaaS. A further wrinkle is produced in that *Open Source Software* (or OSS) can be had for literally no initial capital outlay. Furthermore, systems can be pieced together using components sourced from any or all of the possibilities, creating a mashup that defies categorisation. 

So as not to muddy the already turbid waters, the four groups and the eleven total possibilities are described in general below, followed by a table outlining pros, cons and including a high-level determination of cost, time to value and overall quality. Here then are the eleven routes:

**Figure ZZ. Software Form Factors: Eleven paths to a new system**

![Software Form Factors](https://raw.githubusercontent.com/robertriordan/2400/master/Images/soft_fact.png)

### Eleven paths to a new system - the software form factor
**A: Custom Developed Software (CDS)**

A1. Custom-built system built using the in-house expertise of a dedicated systems development functional area (so called *in-sourcing*).

A2. New custom-built system using expertise from outside the organisation (so called *outsourcing*).

A3. New custom-built system using in-house expertise and built by the actual future users of the system whose job is not systems development but rather in the functional area in which the system will be used (so called *self-sourcing*).

**B: Open Source Software (OSS)**

B1. Custom-built system using code “borrowed” from the open-source community on the internet and built using the in-house expertise of a dedicated systems development functional area (a form of *in-sourcing*).

B2. New custom-built system using code “borrowed “from the open-source community and built using the expertise from outside the organisation (a form of *outsourcing*)

B3 New custom-built system using code “borrowed “from the open-source community and configured/built using in-house expertise of the actual future users of the system whose job is not systems development but rather in the functional area in which the system will be used (while this scenario would be relatively rare, it is a form of *self-sourcing*).

**C: Commercial Off-the-shelf Software (COTS)**

C1. Purchasing a COTS solution and customising it in some fashion to better match the requirements of the functional area in which the system will be used. Think Microsoft Excel with some specific functionality programmed in macros or custom VBA code. The customisation in this case is carried out by a dedicated in-house systems development function (a form of *in-sourcing*).

C2. Purchasing a COTS solution and the customisation is carried out using expertise from outside the organisation (a form of *out-sourcing*).

C3. Purchasing a COTS solution and the customisation is carried out by the end-users of the system (a form of *self-sourcing*).

C4. Purchasing a COTS solution and implementing it with no changes whatsoever (think Microsoft Excel here -- everybody gets a copy on their desktop with the management directive "Now go to it!")

**D: Software as a Service (SaaS)**

D1. SaaS involves essentially “renting” software from a software service provider. If the organisation wants a big enterprise system and doesn’t want the cost or responsibility of buying, installing and maintaining its own copy of the software, said software can be “rented” and provided over the internet. There is little or no customisation here. You take what is offered. If any customisation is available, it's done on a outsourced basis by the service provider.

As you might imagine, there are pros and cons to each approach, a summary of which appears below.

**Table X: Sourcing a system** [need space between title and first row]

| Source type | Advantages | Disadvantages | Assessment
| :- | :- | :- |:-:
| A. CDS | By far the most costly option, but you get what you pay for. CDS provides systems that are specifically optimised to deal with exactly the challenge or opportunity faced by the organisation, and are built upon (hopefully) rigorous analysis, testing and monitoring in ways that are specific to the situation. The organisation isn’t forced to shoehorn into a one-size-fits-all solution and compromise functionality, flexibility and efficiency for the sake of cost. Furthermore, the evolution of the software is under the organisation’s control. The asset value of the intellectual property (IP) embodied in the code is owned and internal and thus controlled. Finally, the organisation retains full control of any data that is implicated in the process. Plenty of upside here. | The old adage that “nothing is free” is nowhere more appropriate than here. While surgery-like solutions to critical issues are the ideal for every organisation, they come at a price. Here, the price is the double-headed monster of real dollar cost and a long time to value (a metric that indicates the elapsed time from spend to reward). There is a risk of obsolescence as well and the organisation owns the system and must maintain it – you own the stack and all the risk associated with the development right through to retirement. Finally, all tech and user support is on your organisation. This can be expensive. Big bucks and a lot of time are the main drawbacks here. If cheap and quick are important, look elsewhere.| ![Custom Designed Software](https://raw.githubusercontent.com/robertriordan/2400/master/Images/CDS.png)
| B. OSS | An increasingly worthy consideration in the systems development mix is the use, whether exclusively or as one of more components of a solution, of open source software. OSS is software that is made available free from initial cost by developers in the community. The code is free to *fork* (copy), modify (create an organisation’s own local version) and implement. Knowing what we do about the time value of money (it’s better to have your money now than at any time in the future) and especially if capital availability is low and/or the cost of capital is high, this is indeed an intriguing opportunity as there is little or no cash outlay at onset. Solutions can be built from existing code or modified as required. Code (at least initially) is supported by a community of developers, providing many eyes on the implementation (but this can be risky as many eyes can also see potential vulnerabilities). Another potential advantage lies in not being tethered to a particular vendor such as SAP or Oracle or Microsoft. OSS is open, and open means freedom to choose. | Again, nothing is free, even free stuff. Gartner (http://gartner.com/webinar/1633714 accessed February 12, 2015) maintains that a sobering 92% of the total cost of systems ownership is accounted for by the maintenance phase (the period during which the system is in use) ergo the initial savings are not as enticing as expected, especially when trying to do maintenance on code that was not written from scratch to be maintainable and is not supported by a software vendor but rather by a vague *user community.*  Moreover, like COTS solutions (see below), such generic code, if unmodified, confers no competitive advantage on the organisation. If it’s a strategic priority to use a system that is tailored to a particular value-generating process, then OSS is not the way to go. In addition, the issue of solution integration must be addressed. A system can’t simply be dropped into place and begin to work seamlessly in the organisation. Larger commercial vendors such as Google or Microsoft will provide such service as part of their development costs were you to hire them to produce a CDS. With OSS, it’s on you to make all the pieces of the organisation perform like a symphony. Penultimately, there just might not be any OSS available to address the challenge at your organisation. There’s no guarantee that anything will fit, and forcing a solution on a problem just because it’s the best solution available poses risks similar to buying COTS (see below). Finally, mixing and matching OSS with other solutions such as CDS and COTS might raise governance issues with open source licensing. Beware and read the agreements carefully. | ![Open-source Software](https://raw.githubusercontent.com/robertriordan/2400/master/Images/oss.png)
| C. COTS | By far the least costly option, and that's always a valid consideration. But paradoxically there's a price to be paid for being cost conscious (see disadvantages). Additionally, it’s likely that industry best practices (efficient and generally accepted, near universal ways of doing things) are adhered to and enforced in commercial software. This should be encouraging to the operations people at your organisation. Finally, it’s likely that the technology is up-to-date. | Given the nature of such software (commercially available, generic systems to solve generic problems), it is unlikely that an organisation's challenges would be efficiently or effectively addressed by such an all-purpose tool. Think Excel. Great tool, tons of power, massive scope, not at all specific to an industry or a functional area. You can do some dentistry with a pair of pliers; question is, would it be efficient or effective? The organisation buying COTS also exposes itself to the risk of vendor viability. What if you buy a system from XYZ Co. and suddenly they go bust? Or are bought out? Additionally, much like OSS, a solution might not exist right off the shelf. So your problem could become framed in the context of the available solution. (For example if a specific solution for your problem is not available, you might find yourself picking a solution to a similar problem because there’s a solution available to solve that *sort of the same* problem. Think of having a piece of wood that you need to cut into two, equal-length pieces. Obviously a saw would work best. However if a saw is not available but a hammer is, the job can be done, but with considerable expense of time and effort and with a much less accurate outcome.) This is obviously not optimal. Finally, retaining competent developers on your staff will likely be difficult. After all, if you’re buying software off the shelf, why would you need developers? Developers develop. Buyers buy. Different people. | ![Commercial Off-the-shelf Software](https://raw.githubusercontent.com/robertriordan/2400/master/Images/COTS.png)
| D. SaaS | This is a fairly expensive proposition, despite there being no development costs at all. The organisation will pay a price for essentially transferring all the application risk to an outside provider, which house and maintain the software at remote locations. There is no need for the organisation to have local hardware or software other than an internet connection and display devices for users. System maintenance (upgrades, bug fixes, etc.) are all the responsibility of the service provider. Service Level Agreements (SLAs) provide guarantees of uptime and availability and the hefty penalties attached to such agreements guarantee that the organisation consuming the SaaS are protected from loss of business arising from unforeseen circumstances (such as outages, system failures, etc.). SaaS offers a very short time to value, includes codified best practices and service providers provide state-of-the-art, up-to-date software, made available on all platforms and devices (from mainframe to smartphone). Finally, the software is available anywhere you have internet access, so distributed systems and scattered workforces (such as the cottage in summer, for example -- oh joy!) are all supported. | Of all the software form factors, SaaS has the highest vendor viability risk. Putting all your eggs in one basket with one vendor can be dangerous. Much like COTS, what if the vendor (service provider) were to suddenly close its doors or go offline? The risk of data lock-in is also high. There is little opportunity to specify the optimal data model (data architecture) for the organisation. You take what you get. This also impinges on data confidentiality issues – how does the organisation protect its IP in terms of its own data when the data is processed and stored off-site? Finally, usage-based pricing could become costly over time if the organisation’s use of the software scales up and is locked-in to the SaaS solution. It is expected that SaaS will grow rapidly in proportion to other forms of system acquisition.| ![Software as a Service](https://raw.githubusercontent.com/robertriordan/2400/master/Images/SaaS.png)
| 1. In-source | More likely to meet user requirements as the organisation’s developers, who will build the system in the case of CDS or configure/modify it in the case of OSS and COTS, are familiar with the organisation's business model and processes. Organisation owns the code and the solution.| On the downside, it’s costly to support such a systems development function and unless the organisation is itself a software house, systems development will not be a core competence. | ![In sourcing](https://raw.githubusercontent.com/robertriordan/2400/master/Images/in_source.png)
| 2. Outsource | Frees up the organisation to focus on its core competence. There is more certain cost control (through performance metrics and contractual obligations) and external technical specialists are likely to use state-of-the-art tools and procedures along with industry best practices. Finally, systems development houses have well-trained staff as technology solutions are *their* core competence| Less likely than in- or self-sourcing to meet user requirements as the requirements must be provided to an outside team with no particular expertise or knowledge of the organisation's business model. This route can be very costly and time consuming. The organisation has no control over the external entity in terms of its survival, potential sale or even going out of business, thus exposing the organisation to significant vendor risk. Moreover, the advantage of doing your own technology in-house is lost to the organisation. Penultimately, this development route is less likely to produce a sustainable competitive advantage as the driver is external to the organisation. Finally, unless specifically guaranteed by contract, the firm that creates the solution (that’s the outsourcer) also owns the code – so the IP is lost. | ![Outsourcing](https://raw.githubusercontent.com/robertriordan/2400/master/Images/out_source.png)
| 3. Self-source | This route is very likely to meet user requirements as the end-users – who are actually creating a solution based on their own needs - are quite familiar with their own processes. Furthermore, the organisation owns the code and the solution, thus the IP remains in house. | Very costly in terms of diverting attention and resources away from the actual work of the end-users (they aren't doing their real job if they are developing a system). Often there is no attention to organisational standards in terms of software tools, protocols, connectivity to larger, enterprise-wide systems or, especially, **security**. Systems development is not a core competence of functional specialists in, for example, the Accounting department, thus systems built by those whose training is in Accounting or Finance or Marketing will not be optimised. Finally, to reiterate, system-wide connectivity, security and privacy are often the most serious issues. And heaven help the organisation if the employee who did the work were to leave for any of the various reasons that people move on, to say nothing about sabotage perpetrated by a disgruntled employee on whose home-made system the organisation has become dependent. Documentation is almost never attempted, let alone completed, in self-created systems. Finally, as Abraham Maslow famously wrote “Give a small boy a hammer, and he will find that everything he encounters needs hammering.” [Interested?](Abraham H. Maslow, 1966. The Psychology of Science. p. 15.) Re-written in its more familiar aphorism form, we often see or hear “If the only tool you have is a hammer, everything looks like a nail.” If the functional analyst building the system is familiar with Excel, then Excel will be her hammer and will be the solution to everything, whether or not it’s the best too for a good solution. | ![Self-sourcing](https://raw.githubusercontent.com/robertriordan/2400/master/Images/self_source.png)

### Buy or Build?
We will consider Rent to be a subset of Buy. That being said, let's start with the *Buy or Build* decision. That's the distinction between commissioning a brand new system using either the in-, out- or self-sourcing option, compared to the decision to buy or rent a commercial product and either customise it or not. 

We begin by posing the question "Should we start from scratch here, rent something or should we buy off the shelf?" There are actually two distinct ways to look at the *B-or-B* question in terms of what is described above in the pros and cons table. We can consider the broad *Buy* category to be comprised of buying or renting anything from outside the organisation, whether it's a custom, ground-up application or an OTS solution that is either customised through outsourcing or not customised at all. So this one broad category involves not using internal resources in any way. 

The other broad category is, obviously, the opposite -- doing everything in-house, whether building from scratch or customising a COTS system using the organisation's resources and personnel.

A useful tool to understand the context of this choice is the Outsourcing Decision Matrix. This is a strategy tool, useful in a wide variety of situations but not necessarily in the B-or-B software systems arena. We'll use the tool to give us an appreciation for the decision process in B-or-B.

The Mindtools website (see the Interested? link below) poses a series of questions much like these: "Which activities should we outsource, and which tasks should we do in-house? For instance, imagine that you work in the healthcare industry. Should you outsource your cleaning staff, or hire in-house cleaners? Would the decision be the same for a furniture manufacturer? If you worked for an airline, would you outsource your in-flight meal preparation, or would you hire cooks directly? What if you managed a luxury hotel?" As I write this chapter, I have just returned from my local Ikea store. My wife and I made a stop in the cafeteria (shopping at Ikea is hungry work) and I found myself wondering if the cooks and serving personnel providing the meals were Ikea employees or outsiders occupying Ikea-owned space and kitchen tools to provide service under contract to Ikea. You get the idea. 

So do you do it yourself or get others to do for you? This is a good question indeed. [[Interested?](http://www.mindtools.com/pages/article/newSTR_45.htm)]

Figure x. Outsourcing Decision Matrix (symmetrical)

![Outsourcing Decision Matrix (Symmetrical)](https://raw.githubusercontent.com/robertriordan/2400/master/Images/outsourcing_decision_equal.JPG)


Figure X shows the matrix in a symmetrical configuration. The idea of this tool is to envision a process or activity undertaken or proposed by a particular organisation and decide what to do with it. We get a measurement on the two dimensions (Contribution to Performance and Strategic Importance) and then plot the process on the grid based on the values of the two dimensions. Simple. 

We will use as illustration here a strategic review process with which we should all be able to relate: that of a University considering the value of an existing academic programme. Universities are increasingly numbers driven (bums in seats) as government funding is increasingly tied to enrolment numbers so schools are required to be agile and to engage in continuous programme review. 

Let’s say that the programme under consideration is a degree in Airborne Fulfilment Logistics – better understood as *delivering stuff using drones*. The school in question currently offers the programme as a joint initiative of the Aerospace Engineering department and the Supply Chain Management people in the Business faculty.  

Now we need to work towards deciding if and how to proceed with the decision of what to do with this drone degree. This is where the tool comes in handy. 

Figure Z. Outsourcing Decision Matrix with proposed process

![Outsourcing Decision Matrix with process located](https://raw.githubusercontent.com/robertriordan/2400/master/Images/outsourcing_decision_system_box.jpg)

As a high-level illustration, let's assume we've got our two metrics (we will discuss these two metrics in detail below but for now, just play along). Now we need to locate the degree programme in the decision matrix based on its measurements on the two axis variables to assist in making the decision on how to proceed. Figure Z shows the metrics on the degree (little blue box), locating it in the upper, right quadrant of the matrix based on it being assessed a score of ~75% on both variables. This locates the process squarely in the "Retain" quadrant, meaning that it's important to competitive advantage and to organisational efficiency. So the organisation should decide to retain the program as is. Now, some detail. 

Upon measurement of the two dimensions, the programme under consideration here will most likely land in one of the four quadrants, (this is guaranteed if, in measuring the two dimension variables, a value of exactly 50% on both is avoided). Note the outcome (strategy) that is represented by landing in a quadrant (clockwise from top left we have: Strategic Alliance, Retain, Outsouce and finally Eliminate). Let’s look in more detail.

First, examine the horizontal and vertical axes, which measure two important variables and range from Low (0%) to High (100%). The vertical axis represents Strategic Importance. A process is considered strategically important if it impacts competitive advantage. Recall from Chapter 1 that competitive advantage is a superiority gained through providing the same value (usefulness or problem-solving ability) as its competitors but at a lower price, or increased revenue through charging higher prices supported by providing greater value through differentiation. [[Definition](http://www.businessdictionary.com/definition/competitive-advantage.html)-accessed February 26, 2015]

In our example, if administration determines that the strategic importance of offering a drone programme is high (administration thinks that it provides an edge in attracting not just good students but specialist faculty and research dollars and that ever-important intangible *prestige*), then they would rank the drone initiative above the 50% level on importance. This signifies that administration feels that, on balance, the programme is a net contributor to fulfilling the school’s strategy. The discussion of exactly how this would be measured is beyond the scope of this text, but must be done in the real world in order to use this tool.

To move forward, let's assume we have some measure of the strategic importance of the programme on a scale from 0 to 100% where higher is more strategically important.

So now we move on to the horizontal dimension, where the impact of the process on operational performance must be evaluated, again on a scale from 0 to 100%. This variable measures the impact on the organisation of the efficiency of the programme: if a program is operationally important but runs poorly, it will impact the organisation in a negative and important way. So the systems involved in supporting and running the programme itself must be efficient and effective. 

(Delete?) Think of the process for fuelling a fleet of busses in a large metropolitan city. If unreliable, slow or otherwise inefficient, the integrity of the city economy could be jeopardised as nothing would run on schedule. On the other hand, a system with little operational significance will not materially affect the organisation's performance one way or the other. Think of the process or washing the exterior of those same busses. Clearly whether a bus is clean or not is nearly as impactful as keeping the fuel tanks full.

Back to our university example. Management must come up with a metric to measure the impact of the degree programme on the overall efficiency of the university. Considerations include whether facilities to house faculty and administration, offices, classrooms, labs, examination rooms, etc. are at a premium or do not exist. In addition, the average salary of professors in the field (professors in rare fields can attract better compensation than others) is high, then this might be factored in. If the programme is more expensive, pound for pound, than other programmes, then it might be decided that overall operational impact might be negative. If, on the other hand, significant synergies (the whole being of greater value than the pieces) are being achieved and/or if space is being efficiently utilised and perhaps if other faculties are finding ways to leverage the drone group, then the scale could be tipped in favour of the degree programme. 

These are complex issues, but here again we assume that a metric exists to measure such impact.

Let’s look at what would happen (what strategy would be followed) from the strategic review according to various hypothetical score combinations. First, a high score on Strategic Importance, coupled with a high score on Contribution to Performance would locate the programme squarely in the Retain quadrant in the upper right. In this case, the university would decide to not only continue offering the programme, but to offer it on campus with full staffing and facilities, keeping it all in house. This programme is a star.

On the other hand, scores coming in low on both dimensions (less than 50% on both measures) would locate the programme in the Eliminate quadrant, making it a prime candidate for elimination altogether. 

These are the two extremes. Let’s say the Operational Performance metric comes in at 75, but the Strategic Performance comes in at 25. This would represent a situation where the programme is efficient and contributing to overall performance, but was not measuring up in terms of the strategic direction of the school. Say the school has a strategy to be the preeminent university in Arts and Philosophy. A highly technical programme in remote drone supply chain fulfillment might not be what the school wants to be known for. So maybe, just maybe, the programme gets cut. Or maybe, just maybe, the school can outsource the delivery of the programme to the private sector. There are any number of private educational training firms out there. The school might want to keep the programme on campus, retaining the operational synergies and the capacity utilisation, but outsource the delivery to the private sector. This would be a difficult sell for faculty. And not likely to fly at all… but the times they are a changing. 
The final possibility is the opposite of the above, where Strategic Performance is above 50%, but Operational Performance is below. So the school likes the programme because it fits with its overall strategy (maybe it wants to be a high-tech hub), but the programme is just not operationally feasible. In this case, a Strategic Alliance might work. The school could look to partner with another local university to deliver the content, perhaps offering to take the operations of an underperforming programme at the partner school in a trade. 

When deciding on an Information System however, it's unlikely that a decision would be made to enter into a strategic alliance with a system solution provider or especially a competitor. Such alliances are formed between two or more organisations in order to solve a particular problem or to take advantage of a unique, perhaps non-recurring opportunity, where all parties to the alliance would benefit. 

But before we throw away this possibility altogether, let’s consider the scenario where the current market is small, but has the potential to grow. Remember that there are two ways to increase revenue in a market: A) take a larger share of the current market by attracting the customers of your rivals, or; B) grow the overall market. In the latter circumstance, partnering with a competitor on strategies that grow the market might not be a bad idea. 

So the organisation could partner with another firm in the same market to *share* the development and ongoing costs of a system that would benefit both parties. This isn’t as far-fetched as you might think. Consider the telecommunications marketplace. In the early years, it was not uncommon for competitors to cooperate on the building of cell tower infrastructure to expand cell coverage. All players benefitted from the expanded availability and then competed on features and price to attract customers from each other to build their market share.

[Interested?](http://www.theneweconomy.com/home/strategic-alliance-ibm-apple)

While a strategic alliance is not likely to happen in the case of a B-or-B decision, this shouldn't dampen our enthusiasm for using the tool. It's still quite valuable as it illustrates the important considerations that need to be taken into account and highlights trade-offs that need to be made when deciding on how to commission a new system.

That's how it works in the ideal world. The real world is a bit messier and more complex.

What are the issues? First of all, we have chosen arbitrary cut points between decision outcomes, located at 50% on each scale. This is aesthetically quite pleasing (yielding four, nice, equally-sized quadrants), but it unlikely to represent reality in all but the most unusual cases. More likely is the organisation having different cultures, priorities, strategies and practices, thus requiring different cut-points.

Figure Y. Decision Matrix examples with different variable cut points

![Outsourcing Decision Matrix Mashup](https://raw.githubusercontent.com/robertriordan/2400/master/Images/outsourcing_decision_mashup.jpg)

Examining Figure Y, we see organisations imbued with different realities. A particularly lean organisation would have a very small upper, right quadrant (the Retain area), preferring to outsource, partner or eliminate all but the most essential of processes. The cut points for both *strategic importance* and *contribution to efficiency* could be moved up to 75% or even 80 or 90%. Such organisations are sometimes referred to as *virtual organisations,* existing as only a core set of processes with no real physical space, having outsourced, partnered-off or cut everything not considered absolutely essential.

The recent and astoundingly swift rise of Web 2.0 communications and collaborative technologies, along with e-commerce, secure credit card transactions and teleworking, coupled with the rapid rise of the service sector, have led to the rise of such *virtual* (not bricks and mortar) organisations. This is how Amazon started way back when.

Don't confuse this sort of virtual organisation with a *virtual corporation*, which is really a form of strategic alliance. [Some sources](http://www.allbusiness.com/glossaries/virtual-corporation/4960651-1.html) muddy the water a bit on this.

[[Interested?](http://www.economist.com/node/14301746)]

Let's then take our new-found expertise in decision making and translate it into a more difficult situation which, while sharing some similarities with the outsourcing decision challenge, has more and varied inputs to the process.

To make the decision about how to commission a new system in an organisation, a series of determinations need to be made. The *consideration* column can be seen as the *characteristics* of the system or of the organisation considering the system.   

Table XX. Buy-or-build considerations

| Consideration | Build if | Buy if |
| :- | :- | :- |
| 1. System size and complexity | The requirement is for a small, simple and *ad hoc* system | You need a big, complex system, or if an adequate COTS solution exists and is cost-effective |
| 2. Strategic necessity | Not strategic and/or limited operational impact | Strategically important and/or significant operational impact |
| 3. Timeline | Small with plenty of time to get it right | Medium to large with a tight delivery timeline |
| 4. Uniqueness | Unique or proprietary or might expose a trade secret or competitive advantage | Ordinary, garden-variety with no secret sauce |
| 5. In-house talent | Have a dedicated, well-funded and persistent systems development resources | No real competence in development |
| 6. System footprint | Small, restricted impact on a single function or process | Impacts a larger number of functions and/or needs integration with an enterprise system (such as SAP) and/or must be scalable and robust |
| 7. Lifespan | Short | Long |
| 8. Compliance | Not required to be compliant with external agencies | Requires external compliance such as SOX or Bill C198 among others |
| 9. Stability | Industry, market, competition and business practices are stable and glacial | Industry, market, competition and business practices are volatile |

There are plenty of compliance standards, and many agencies (especially governmental) have their own set of rules.  [[Interested?](http://www.cihi.ca/CIHI-ext-portal/internet/en/Document/standards+and+data+submission/standards/MIS_FAQ)]

There's lots to consider when deciding to B-or-B. [[Interested?](http://www.techrepublic.com/article/buy-vs-build-six-steps-to-making-the-right-decision/)]

## What makes a quality system?
Regardless of the decision to either buy or build, there are certain imperatives in system building. Let's examine them now.

Quality is an aggregate function of how well a system meets each of the following characteristics. Each is important in isolation, but the relative weight of each factor in determining overall quality is context dependent (and isn't *everything*?). Not to put too fine a point on it, but in one context at a certain point in time for a certain organisation, *efficiency* might take precedence over *security*, say. At another point in time, in another circumstance, the reverse might be true. So the relative weights of the system quality factors below can change over time, both within and between organisations and are sensitive to context.

Here are the factors:

- **Exhaustiveness** — A system is exhaustive if it *addresses all requirements* specified by analysing existing systems (if extant) to discover shortcomings, by examining the organisation's needs including their strategies, and by polling potential system users in the *Requirement Analysis* phase (see later discussion on SDLC and others) of a proposed systems development project.
- **Reliability** — A system is reliable when it operates in *predictable and consistent fashion* no matter the demand load placed upon it. It is said to be *scalable*, both up and down, if it robustly responds to changes in demand such that there is minimal impact on other important metrics (such as accuracy and efficiency). "It takes a licking and keeps on ticking." to borrow a 1950-1960s and again in the 1990s *Timex* watch ad in which wristwatches survived various staged torture tests. Systems must have the capacity to handle peek workloads while ideally being able to scale back when capacity isn’t required. And they must do so without sacrificing reliably.
- **Accuracy** — A system is considered to be accurate when it produces *predictable and verifiably-correct outputs* when executing its required functions. So 2 + 2 is always 4. This requirement speaks to data and process integrity. 
- **Efficiency** — A system is efficient if it produces outputs that are *more valuable than is the cumulative cost of the required inputs* to the system. The greater the spread between the value of outputs and the cost of inputs, the more efficient is the system. This is the basic and most important test of the value of ICT. A system must alter the relationship between inputs and outputs in a positive way by either shrinking required inputs or growing the volume or value of outputs. The spread between the cost and the benefit should be positive, non-zero and subject to continual scrutiny and improvement.
- **Security** — A system is considered secure if access to the system itself, to any required inputs, and in some cases the system outputs, are* protected from unauthorised access and tampering*.
- **Usability** — A system is considered to be usable if it allows users to complete their work with *a minimum of error, wasted effort or frustration*. This obviously impinges on the characteristic of efficiency. Much thought and effort has gone into the area of user experience design (or UxD) in the system design community of late (more on this later in this chapter).
- **Maintainability** — A system is maintainable if its program code is both written according to accepted industry standards and is well documented such that* errors can easily be detected and corrected* (see transparency below), and new features can be added (or existing ones deleted) with minimal impact on the metrics of other important system characteristics (such as efficiency). You must be able to fix what’s broken.
- **Transparency** — A system is transparent if it is designed in such a way as to allow *metrics on its other characteristics to be tested and gauged*. Thus a system should afford testing of reliability, accuracy, etc.
- **Availability** — A system is considered available if it is *online and readily accessible* to do the job for which it was designed. 
- **Recoverability** — A system is recoverable if it can be brought *back online quickly* and with minimal data loss following a system outage caused by any type of problem. This requirement demands that a disaster recovery plan be in place and enforced.
- **Interoperability** — A system is considered to be interoperable if it *plays well with others.*  In other words, it fits well into the infrastructure and is a seamless player with whatever other processes or systems of which it is a part in the organisation. A good system should be virtually transparent – just quietly goes about doing what it is tasked with doing. 

## What's at stake?
<a name="fail"></a>
Large organisations spend a lot (a LOT) of their resources on systems development, training and maintenance. How much? One study published by McKinsey claimed that organisations spend around 50% ([some report](http://www.mckinsey.com/insights/business_technology/enhancing_the_efficiency_and_effectiveness_of_application_development) up to 60% in the US) of their total IT budget on application development. But even given the big bucks spent, "... the quality of execution leaves much to be desired." Furthermore, they offered that "A joint study by McKinsey and Oxford University found that large software projects on average run 66 percent over budget and 33 percent over schedule; as many as 17 percent of projects go so badly that they can threaten the very existence of the company." Yikes!

[[Interested?](http://www.mckinsey.com/insights/business_technology/achieving_success_in_large_complex_software_projects?cid=other-eml-ttn-mip-mck-oth-1412)]

## Building a system?

To the quality metrics introduced above must be added and separate but related dimension -- that of timeliness of delivery. Often, organisations face problems that demand immediate solution or are presented with opportunities the window to which is fleeting and fast closing. So solutions need to be timely.

A McKinsey/Oxford University [study](http://www.mckinsey.com/insights/business_technology/developing_talent_for_large_it_projects?cid=other-eml-ttn-mip-mck-oth-1412) reported that "... 71 percent of large IT projects face cost overruns, and 33 percent of projects are around 50 percent over budget. On average, large IT projects deliver 56 percent less value than predicted."

So it's clear that there are real challenges in system development and in the related field of Project Management as it impacts large software development (more on this later). Clearly the software development community is aware of their quality and timeliness issues. More needs to be done, however, to improve on the underlying dimensions that give rise to these critical metrics.

How then is development actually accomplished? Whether the decision is to in-source or outsource, there are several paths that systems development can take. The general model of *systems* development, of which *software* development is a specific sub-species, will be discussed at length below. Here is the *Software Development life Cycle*.

Figure TTP. Software Development Life Cycle

![Software DLC](https://raw.githubusercontent.com/robertriordan/2400/master/Images/Software_DLC.png)

<a name="soft_dlc"></a>
Now need some yaddy yadda here! 

## Systems development methods
As systems development evolved from an art to a science and increasingly came under the scrutiny of project management (seeking predictable structure) and cost accounting (fixated on ROI), methods were needed to ensure that systems were delivered on time, to specification, were efficient, accurate and maintainable, thus providing the longest possible period of trouble-free operation and thus a decent return on the significant investment put into them.

Indeed, methods were created and rigour was added and metrics were devised. The basic steps in systems development do not vary much from method to method. Their arrangement, sequencing and whether they can be revisited once complete (or indeed if they are ever completed at all) is what creates the various flavours of systems development. It is on a tour of many such methods that we now embark. But first, the progenitor.

### The Systems Development Life Cycle (SDLC)

The first such formalised method was the Systems Development Life Cycle, a so-called *waterfall method*.

The SDLC is referred to as a *waterfall method* since its steps resemble a waterfall; in order for water to reach the bottom of a waterfall, it must pass along the entire span between the top of the falls (system conception) to the bottom of the falls (system retirement). The water can't skip any of the distance along the way. Equally, once the water has reached any fixed point, there's no going back. It's all one way, and the only way is to finish the plunge to the bottom.

Figure ZZ shows the Systems DLC (as opposed to the Software DLC introduced earlier [(take me there)](#soft_dlc)) illustrating the seven phases and accompanied by a whimsical estimate, through images, of the cost of finding and fixing errors at the various stages of the cycle. Image, if you will, the associated animal appearing suddenly, unannounced, in your home. What would it take to remedy the situation? The fly is a simple nuisance. Image the structural havoc to your home of having to deal with a 6,500+ kg. bull elephant in your kitchen.

That’s the order of magnitude of issues faced by software developers who use the SDLC. The US space agency NASA estimates that, on average, it can take upwards of 100 times the resources to fix an error discovered after delivery (implementation) than if the error were discovered in the requirements or early design phase of development. Some estimates are up to 1,000 times. 

[Interested?](http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20100036670.pdf)

Figure XCB. SDLC Phases 

![SDLC Phases](https://raw.githubusercontent.com/robertriordan/2400/master/Images/SystemsDLC.png)

Let’s take a quick look at what happens during each of these eight phases of the SDLC. 

| Phase | Activities | Cost of errors |
| :-: | :- | :-: |
| Inception & feasibility | This is where the idea for a new system bubbles up from the organisation through innovation, error detection or challenges arising from competitors or regulatory or market changes. The possibility of a new system arises here where it gets a preliminary scan (see Chapter # where we discuss governance and decision making) and the general parameters of the organisational response are set. This is a gateway stage. The questions at this stage are “Here’s the situation. Can a system help us here?” Approval is critical. A no-go means end of discussion. | ![Fly](https://raw.githubusercontent.com/robertriordan/2400/master/Images/fly.png) |
| Requirements analysis | Given a preliminary go at the inception stage, analysts of both types (business and systems) begin to gather the requirements for the system. What problem is the system supposed to solve? Who will the users be? What inputs are required and what are the expected outputs? Fairly broad, general requirements lead to quite specific details as analysis and fact finding progress.  Two types of requirements are gathered in this stage: Functional (what the system must do), including:  business rules, transaction corrections and adjustments, administrative functions, authentication, audit tracking, external interfaces, certification requirements, and reporting requirements, among others. The second set of requirements revolve around non-functional requirements (what qualities the system must have and to what standards is it subject - see discussion in this chapter entitled *What makes a quality system?*) Requirements here include: scalability, capacity, availability, reliability, recoverability, maintainability, serviceability, security, regulatory, manageability, environmental, data integrity, usability, interoperability and performance. [Interested?](http://usabilitygeek.com/requirements-gathering-user-experience-pt1/) | ![Mouse](https://raw.githubusercontent.com/robertriordan/2400/master/Images/mouse.png) |
| Design | There are two distinct types of design occurring in the design phase: Logical and Physical.  In the logical design sub-phase, analysts are concerned with high-level models and abstract representations of data flows and the inputs and outputs of the system. Data might just be referred to as “data” with no attempt to describe the characteristics at all. This phase if conceptual, as in going no deeper than the “concept” of a system and what it might broadly be tasked with in terms of the requirements gathered in phase 2. This is the “what” of system design – just broadly *what* a system will do, but not *how* it will be accomplished. The physical design activity, on the other hand, gets into the nitty-gritty of exactly *how* things will get done. Data and storage and process and input/output details are considered down to the most minute detail. Tools that analysts use in this process include Data Flow Diagrams (DFDs) and Entity-Relationship Diagrams (ERs or ERDs) and Unified Modelling Language or UML ([Interested?]( https://www.youtube.com/watch?v=OkC7HKtiZC0)) diagrams.. This is where the system is *scoped out* in preparation for building it, a phase that requires exact detail. Details such as exact inputs, process and output displayed and/or stored are carefully specified here. All the external actors who or which interact with the system are defined here, and their behaviour modelled. It is at this stage that user interfaces are designed. This is one of the most critical and creative pieces of software design. See the discussion in this chapter regarding Ux design. | ![Bunny](https://raw.githubusercontent.com/robertriordan/2400/master/Images/bunny.png) |
| Development & coding | In this phase, the design emanating from the previous phase is programmed into actual software. Hardware such as computers, servers and even satellites, if required, is purchased. Service contracts for things such as cloud storage are negotiated and initiated. Software systems such as commercial databases are purchased, installed and configured while code is being written to access them. Requirements are put into action. Coding is accomplished using a language appropriate to the target environment (mainframe, desktop, Windows, Mac, mobile, Android, iOS, etc.) using any one or more of the many, many coding languages available. In addition, one of the two types of documentation is produced here – *technical documentation*. This documentation details how the system works on the inside. It provides such detail as object models (if using object-oriented design), data flow diagrams (DFDs), UML diagrams (UML is Universal Modelling Language), ERDs (Entity-Relationship Diagrams and other technical documents which would inform those doing maintenance on the system when it is implemented. | ![Cat](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat.png) |
| Testing & verification | The testing phase measures the actual versus expected outcome of the system. The outcome expectations are based on the system requirements elicited in the requirements stage. The goal of testing is to find and fix unexpected outcomes when actually executing the system as built in the design phase.  There are numerous types of testing, culminating in *beta testing* (where sometimes the public is invited to use a system for free and to report bugs to the developers) and *acceptance testing* (where the actual users of the system, which is in production-ready mode, are tasked with giving their final approval, or acceptance). While the SDLC (as a waterfall method) is no given to iterative development (test, find, fix, test…), testing is often conducted in this fashion. So quite unlike other phases of the SDLC, where discovering serious errors necessitates moving all the way back to the feasibility stage and a restart, testing often uncovers minor issues in coding, for example, that can be fixed quickly and without resetting the whole project. A specific type of testing, in fact, is dedicated to this iterative process. This type is *regression testing* wherein the system is re-tested to make sure that fixes to previous bugs have not introduced errors in previously error-free code. The beat goes on.  The importance of testing cannot be overstated. It is at this stage that *user documentation*, the other important type, along with *technical documentation*, is created. User documentation is all about how to interact with the system from a user perspective including how to install, troubleshoot, maintain and use the system such that your goals as a user are satisfied by the system. [Interested?]( http://www.waterfall-model.com/sdlc-test-phases/)| ![Dog](https://raw.githubusercontent.com/robertriordan/2400/master/Images/dog.png) |
| Implementation & integration| In this phase, the fully-tested system is put into production in the target environment. The choice of how to *transition* to the (often) new system is an important one. See Table ZZZ for a comparison. Additionally, the new system must often be integrated with existing systems and workflows in the organisation. This is especially, though not exclusively, important when implementing a COTS system. All the pieces need to work together in order to create value. | ![Horse](https://raw.githubusercontent.com/robertriordan/2400/master/Images/horse.png) |
| Maintenance & Evaluation| In this stage, the system is subject to monitoring to ensure that it continues to create value and meet the expectations of the organisation and the users of the system. Small, incremental additions, deletions, fixes and improvements are made on an ongoing basis. Recall that Gartner has reported that 92% or the total cost of system ownership is accounted for by activities in the Maintenance phase. | ![Elephant](https://raw.githubusercontent.com/robertriordan/2400/master/Images/elephant.jpg) |
| Retirement | Finally, as with everything, the end eventually arrives. When a system cannot be further patched or tweaked and has stopped creating value (for whatever reason), it’s time to retire it and move on. Note the line in Chart XX points directly back to Inception and Feasibility. Time to start again. At this point, several critical tasks must be undertaken, including securing the input and output data, both current and historical, involved with the system. Retiring a system shouldn’t retire the data associated with it. | ![Headstone](https://raw.githubusercontent.com/robertriordan/2400/master/Images/tombstone.png)|

And here’s Wally’s take on implementation:

![Wally](https://raw.githubusercontent.com/robertriordan/2400/master/Images/wally_implementation.PNG)

http://dilbert.com/strip/2015-04-22

### System Conversion - putting the new system to work
This process involves the replacement of an existing system with a new one. Think of the new system in the broadest possible terms, as a new system might also involve changes to infrastructure, processes, networks and even personnel. Such deeply technical details would not likely be specified by business analysts but rather must be discovered and specified by systems analysts in the requirements analysis phase of the SDLC. Regardless of when or whom, all requirements must come together and be satisfied in the implementation and integration phase. 

Generally, the conversion phase applies only to the situation where an existing system is being replaced by a new one. An entirely new-to-the-organisation system would likely be converted with the *plunge* method (see below). 

Let’s take a look at the four most common methods. Figure RM below shows the methods graphically.

![Figure RM. Conversion methods](https://raw.githubusercontent.com/robertriordan/2400/master/Images/conversion.png)

Let’s examine each in light detail. The metrics in the rightmost column represent subjective assessments of the Risk associated with the conversion method (the column with the little traffic cone at the bottom) and the cost of the method (with the little $ in the bottom of the column). Recall from our discussion of quality metrics in the sourcing section of this chapter, the height of the colour in the column represents the absolute top of the range for risk or cost, while the density of the colour (always more dense near the bottom) represents the likeliest value of the measure. To calibrate, note that the least risky method is parallel , but it’s also the costliest. The polar opposite is true for the plunge method. 

It appears that there is a negative relationship between risk and cost. And that’s true in general. As nothing is free (even free stuff), a reduction in risk to the organisation will cost money. You just can’t escape. Willingness to accept more risk will be less costly… if things go well. 

Table ZZZ. Comparison of conversion methods

| Conversion Method | Description | Good | Not good | Risk/Cost 
| :- | :- | :- | :- | :-: | 
|Parallel | Both systems run at the same time for a period of time. | When the existing system performs a critical function in the organisation and you can’t do without the output it provides and/or when there is an intolerable level of risk that the new system might fail. | When time is of the essence and the new system provides something that the current one cannot. Also when there just aren’t enough resources available to do both jobs at the same time (i.e., if there aren’t enough staff to prepare a critical input file in two ways). | ![Parallel](https://raw.githubusercontent.com/robertriordan/2400/master/Images/parallel.png) |
|Pilot| The system is incrementally implemented in subsets of the organisation. | When the organisation does not critically need the output provided by the new system and more importantly if the system is large enough to allow it to be phased in. If the organisation is large enough to warrant an enterprise-wide system, and the system will be implemented in many functional areas, then a phased conversion could be carried out in one functional area at a time (for example, first in Accounting and then in Finance and then in Sales, etc.). Another scenario could see some members of one functional area using the new system while others use the old one. Finally, if the organisation is large enough to have multiple locations doing roughly similar work (a series of auto manufacturing plants, for example) then the conversion could be phased in one plant at a time, or one country at a time, etc. This type of conversion lets the organisation gauge the success of the conversion with less exposure to risk than would be the case under Plunge conversion, for example (see below).  | When either the reach of the system or the size of the organisation do not allow for enough scenarios. In addition, if all areas or personnel or installations of the company  work in tight unison it might be difficult to find the opportunity to differentiate the work process.| ![Pilot](https://raw.githubusercontent.com/robertriordan/2400/master/Images/pilot.png) |
|Phased | Different pieces (modules, functions) of the overall system are implemented at different times. | When the system is large enough and the pieces are different enough to warrant splitting up the system. | When the benefits of the system (or even the integrity) would be compromised by piecemeal implementation. Some systems are all or nothing. In addition, it could be quite expensive to  repeatedly integrate pieces of a new system with existing infrastructure and processes, let alone train users many times as new pieces become available. | ![Pilot](https://raw.githubusercontent.com/robertriordan/2400/master/Images/pilot.png)    
|Plunge | Lights off, lights on. Out with the old, in with the new. Retirement party on Friday. Monday we start the new system. | This is good when cost is critical or when it’s not operationally feasible to run two systems at one. Also when the output of the old system is not needed in its current form and moreover, if time is of the essence – you need the new outputs ASAP or if there is a drop-dead date for implementation that cannot be changed (funds might disappear, for example, if the new system isn’t implemented before the end of the fiscal year). Finally, it’s good in situations where it’s relatively easy to return to the old system should the situation warrant.  | When you need the output from the existing system, or when it’s not clear what the outcome of the conversion will be (risky). | ![Plunge](https://raw.githubusercontent.com/robertriordan/2400/master/Images/plunge.png)   

Regardless of what you might think of the SDLC in terms of its viability for creating systems, the stark truth is that all methods (more of which we are about to discuss) must engage in each and every phase of the SDLC in order to produce a software system. It is in the emphasis, sequence, frequency, timing and duration of time spent on each phase that is the major differentiator between the various methods of systems development. 

Proponents of the SDLC and other waterfall variants (there are fewer and fewer of them all the time) will argue that adopting these methods have the advantages of forcing systems developers to fully understand the system requirements before embarking on a development project. Additionally they felt that there were savings in time, effort and resources (including money) to be realised from this rigid, un-adaptive process.  Change was not to be tolerated. Figure it out at the start, then build it. 

But there is an old adage among developers: “Often users don’t know what they want until they see what they don’t want.” This is a compelling argument for the Prototyping Method, presented next. 

### Prototyping

Prototyping takes a different view from the SDLC. The principle behind prototyping is that it allows the eventual users of the software system to get early and then frequent insights into the current design of the system by allowing users to actually use the it before completion, rather than having to either read about system features and processes or, in the worst case, to wait until they are tasked with acceptance testing of the finished product. By then, for all intents and purposes, it’s too late. The elephant is in the room. 

Prototyping, as an iterative (built in smaller pieces and shown to the users for approval) process, can thus better protect against the potential catastrophe attendant upon *finding the elephant*. But make no mistake. While more unlikely given the repeated, iterative exposure of the system to its eventual users, there still exists the possibility of finding that pachyderm at the end.

Furthermore, prototyping is also often used to *try out* improvements to a system that were not part of the original specification but which *occurred to* the developers (or users) in the process of building. In this sense, it is much closer to the Agile methods we will discuss later. For now, back to prototyping. 

Figure LJ. The prototyping process

![Prototyping](https://raw.githubusercontent.com/robertriordan/2400/master/Images/proto.png)

Figure LJ provides a pictorial overview of the prototyping process as it relates to software development. First, note that there are no more bunnies, cats or dogs as a cost of finding errors. Prototyping replaces them with mice because there's not as much at stake at each iteration. The exposed and under-scrutiny part of the system is smaller, the changes more easily made and the investment in development much smaller. This is an improvement over the SDLC. 

Next, note that the red lines connecting steps 3, 4 and 5, unlike in the SDLC, can be repeated (or iterated) as often as necessary until the evaluation at step 6 branches to step 7, effectively exiting the prototype loop when the system is ready for implementation. The obviously pivotal step is #6, where in addition to branching to step #7, the outcome of the prototype evaluation can branch to either a refinement of the current design (step #3) or to a revisit of the system requirements if the prototype is introducing a refinement (step #2). So it’s really all about the user feedback. This is an important advantage for prototyping.

### The prototyping process for software development
1.	Project inception and feasibility stage is a given. We don’t just start producing a system out of nowhere. There must be demonstrated need and management approval if working in a large organisation.
2.	Perform a basic requirements analysis, but not as in-depth as in the SDLC. Often, the riskiest bits of the system are modelled early. If you can do the hard stuff, the easy stuff will fall into place. So we get the nitty-gritty details; the make-or-break stuff.  Often, difficult but manageable details such as security are ignored in the early stages. 
3.	A prototype is developed, often including only user interfaces in the earliest stages. This is referred to as Horizontal Prototyping (discussed below). In later stages, Vertical Prototyping is used to drill down deep into the system to model the full functionality of a feature or required process. 
4.	The prototype at whatever stage it’s currently at is shown to the clients (end users) to elicit feedback. 
5.	The feedback provided in step 4 is used to revise and enhance the prototype. New features or screens are added in an incremental fashion and we return to step 3, continuing in this fashion until a deliverable system is produced. 

Usability Engineer Jakob Neilson described the various types of prototyping in his 1993 book entitled Usability Engineering (Academic Press Inc.). Two are relevant to us in this context: 
### Horizontal 
A user interface prototype is referred to as a horizontal prototype. Such a prototype provides an overview of a complete system or a significant subsystem, with an emphasis on how the user uses the system rather than how the system processes user input. Such prototypes are useful to confirm the logic and flow of user interfaces and setting the broad parameters of what the system (or sub-system) is expected to do. Furthermore, they can help in developing some metrics around anticipated time and resources required to deliver the full system. Finally, such prototypes often have the benefit of securing *buy-in* from decision makers at the organisation as they can see and more tangibly grasp the overall scope, and thus value, of the proposed system. 
### Vertical 
A vertical prototype is an in-depth elaboration of a single process, subsystem or function. Such prototypes are useful in discovering detailed requirements, such as the “risky bits” of a system and to demonstrate that they can be accomplished.  The benefits of vertical prototyping include determining details of data modelling (database design)and in getting a handle on processing volume requirements (how many transactions, or basic units of work) a system is required to be capable of doing, 
Further distinctions are made between Throwaway and Evolutionary prototypes. The former is just as its name imp[lies, and is not as often used in software prototyping as the latter, which retains its basic structure and functionality throughout the process to become the final system.
### Prototyping Benefits
Prototyping has several benefits: 
1. Valuable *user feedback* is elicited early and often
2. *Requirements can be more easily verified*, tweaked, added or dropped
3. Metrics around *cost and time are brought into focus* iteratively
4. Users are *much more likely to buy into the system* and support its use when it goes into production as they had a stake and a voice in its development – they feel ownership of the system because they helped craft it and accepted it at each stage
5. *Training requirements for users are significantly reduced* they have been exposed to the system often and have trained themselves on its use and indeed directed the development in part
6. Prototyping is especially useful when systems involve *extensive interaction between computer and user* (have extensive Human-Computer Interaction or HCI)

Prototyping also has some drawbacks. These include, but are not limited to:
1. *Insufficient analysis*: The focus on a limited prototype can distract developers from properly analyzing the complete project.
2. *User confusion of prototype and finished system*: Users can begin to think that a prototype, intended to be thrown away, is actually a final system that merely needs to be finished or polished. 
3. *Developer attachment to prototype*: Developers can also become attached to prototypes they have spent a great deal of effort producing; this can lead to problems like attempting to convert a limited prototype into a final system when it does not have an appropriate underlying architecture. 
4. *User attachment to the features of a prototype:* Users might become accustomed or attracted to features that were included in a prototype for consideration and then removed from the specification for a final system. This can lead to misunderstanding and conflict.
5. *Excessive development time of the prototype*: A key property to prototyping is the fact that it is supposed to be done quickly. If the developers lose sight of this fact, they very well may try to develop a prototype that is too complex. 
6. *Expense of implementing prototyping*: the startup costs for building a development team focused on prototyping may be high. Many companies tend to just jump into the prototyping without bothering to retrain their workers as much as they should.
7. Prototyping can lead to both *scope creep* and *feature creep*. 

[Interested?]( http://en.wikipedia.org/wiki/Software_prototyping#Disadvantages_of_prototyping)

Table RM provides a comparison between SDLC and prototyping on some important dimensions. 

Table RM. An evaluation of the SDLC and Prototyping 

| Condition | SDLC is | Prototyping is |
| :- | :-: | :-: |
| When user requirements are poorly specified | Poor |  Excellent |
| When developers are using unfamiliar technology | Poor | Better |
| When developers are using proven technology | Good | Better |
| With complex projects | Good | Good |
| When delivery deadlines are tight | Poor | Good  |
| When technology is changing rapidly | Poor | Good |
| When continuous management buy-in is crucial | Poor | Better |
| When user buy-in and support is critical | Poor | Excellent |
| When audit trails and multi-level sign-off are critical | Excellent | Poor |
| Where scope creep or feature creep need to be carefully managed | Excellent | Poor |

### Feature Creep and Scope Creep
Any time clients/users ask developers to increase the amount of work a system will do, or to include features that were not specified in the original system requirements, * creep* is at work. Project managers and developers must be continuously on the lookout for such creeps, as they are everywhere. Each tiny little addition, without a commensurate increase of time allotted, resources allocated or quality expected, leads the project one tiny step closer to failure. 

In the broadest sense, an example of scope creep might be if users were to ask the developers of Microsoft Excel (a spreadsheet) to also provide the capability to produce manuscripts (a word processing function). The scope of work is therefore much broader than what a spreadsheet is normally expected to do. Feature creep, on the other hand, could be illustrated by users asking Microsoft to include a feature in Excel whereby every time a user entered a valid email address in a cell, that email address is added to the user’s contact list. Nice feature. Not in the original specs. Nothing to do with a spreadsheet’s core functionality. 

## Agile
The SDLC and, to a certain extent prototyping, represent the so-called *heavyweight*, waterfall-oriented methods, which critics have called ponderous (cumbersome), sclerotic (rigid) and over-managed (too many rules to follow). Such shortfalls led to the development of lightweight agile software development methods in the mid-1990s. 

Early implementations of agile methods include Unified Process in 1994 (specifically implemented as the Rational Unified Process or RUP following IBM’s purchase of Rational Software in 2003), Scrum in 1995, Extreme Programming (EP) in 1996, and others. 

These methods are now collectively referred to as Agile Development following the publication of the [Agile Manifesto](http://en.wikipedia.org/wiki/Agile_software_development#The_Agile_Manifesto) in 2001.

Figure NA. The generalised Agile process 

![SDLC Phases](https://raw.githubusercontent.com/robertriordan/2400/master/Images/agile.png)

Note from Figure NA the emphasis on delivering working software at the end of each rapid iteration (it should take no longer than a month – and often much, much less time -- to traverse from top to bottom of one iteration in agile), then moving back to the beginning after evaluation by users. This is much closer to prototyping and clearly differentiates from the SDLC where stages are begun and finished and never revisited unless a catastrophic error causes a complete reset. This is the main contrast between the two camps. The SDLC is predictive – all is known beforehand and the process doesn’t ever vary. Agile methods are more adaptive and flexible, as we’ll see from or discussion of the Scrum flavour of agile, below.

Lean is a flavour of Agile, and has been extended or adopted by other movements in the business world. As Gaping Void designer/artist/guru Hugh MacLoed and business partner Jason Korman imply in the image below, it's always the destination that should drive the agenda, rather than how to get there. Going on a trip, you wouldn't say that you are "...going in a Toyota", but rather that you are "...on our way to Disneyland", for example. Here's the gapingvoid.com feature for April 10, 2015:

Figure MC. The Lean process 

![Gaping Void Lean](https://raw.githubusercontent.com/robertriordan/2400/master/Images/lean_gaping_void.PNG)

Here's what MacLoed writes in the accompanying text to the daily email from gapingvoid.com regarding the graphic:

"I never know from the start, what a drawing is going to look like when it’s finished. I just make it up as I go along. And that’s perfectly normal. Nobody ever asked Beethoven, “So when you get around to writing that Ninth Symphony, what notes are you going to use? How many times? In what order?” Of course not. All one has is a feeling that if you set off in a certain direction, something solidly good will happen. Maybe. And what’s true for symphonies is also true for businesses. The final project is never the same as when you first imagine it. The idea is changed the minute it comes into contact with execution. You just kinda know “what” is going to happen, without actually knowing what exactly is going to happen.
 
"This loose relationship with the word, “What” is at the core of our friend, Eric Ries’ “Lean Startup” movement. Not waiting around to know “Everything”, because that is impossible. Instead, you just start. And keep going. And stay light on your feet enough (i.e. low burn rate) in order to change directions on the dime (i.e. “pivot”) if you need to. Worry about the music, and the notes will take care of themselves."

Lean is much more than about systems development. If you get intersted below, you'll note striking similarities to prototyping. Agile and Lean both owe some props to prototyping as they are descended from the general principle of providing frequent and rich feedback, and changing course based on the feedback, that is the foundation of prototyping.

[[Interested?](http://theleanstartup.com/principles)]

Agile has many devotees, at least partly because the principles of the Agile movement are simple, straightforward and compelling. 

[Interested?](http://agilemanifesto.org/principles.html)

The contrast with SDLC are added by the author in the SDLC reflection column:

Table TB. Agile principles with comparative reflections on the SDLC 

| Agile principle | SDLC reflection |
| :- | :- | 
| Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. |SDLC delivers at the very end of the process. |
| Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. | Change is discouraged given the high cost of returning even to the previous stage, let alone to the start of the process. |
| Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale. | SDLC delivers only as early as the testing phase, close to the end. |
| Business people and developers must work together daily throughout the project. | Business analysts are involved only early in the process to provide requirements but then not again until testing, late in the game. |
| Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. | Systems developers are divorced from the business process and develop in a black box without continuous feedback from the business side. |
| The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.  | Little communication between developers and developers and even less between developers and business analysts. |
| Working software is the primary measure of progress.  | Adherence to milestones, cost certainty and incremental approval are measures of success. |
Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. | Pave of development is dictated by external signoff and resource allocation. |
| Continuous attention to technical excellence and good design enhances agility. | Parameters are determined at project outset and not reviewed continuously. |
| Simplicity -- the art of maximizing the amount of work not done- is essential. | Steps are undertaken because they must be followed in order to get signoff and are divorced from agility and/or improvement. |
| The best architectures, requirements, and designs emerge from self- organizing teams. | Teams are organised around resource efficiency and cost certainty and not product excellence. |
| At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. | Little internal reflection; emphasis is on conformity and hitting milestones on time, at or under budget. |

This might seem a stinging indictment of the SDLC. Indeed in many ways, the SDLC has outlived its usefulness, given modern technology and the growth of the internet as a medium for delivering applications. There are obvious differences between the monolithic applications popular in the heyday of the mainframe era. 

But many organisations, especially those that are large and bureaucratic in nature (governments, for example), find comfort in the rigorous rules and milestone reviews and signoffs. We mustn’t sell short some of virtues of the SDLC. This being said, let’s return to Agile.

Here is what the Agile Alliance(agilemanifesto.org/principles.html) says about its values:

Agile Values: 

1.	**Individuals and interactions** over processes and tools 
2.	**Working software** over comprehensive documentation 
3.	**Customer collaboration** over contract negotiation 
4.	**Responding to change** over following a plan 

They close with “That is, while there is value in the items on the right, we value the items on the left more.”

[Interested?]( http://www.ambysoft.com/essays/agileManifesto.html)

Let’s now take a brief look at Scrum, a popular agile software development method model. “Agile and SCRUM are related but distinct. Agile describes a set of guiding principles for building software through iterative development. Agile principles are best described in the Agile Manifesto. SCRUM is a specific set of rules to follow when practicing agile software development.”

[Interested?](http://stackoverflow.com/questions/1586928/how-different-is-scrum-practice-from-agile-practice)
Following is from the book entitled *Scrum: a Breathtakingly Brief and Agile Introduction*, 2012, Dymaxicon, ISBN 10: 193796504X. This overview of roles, artifacts and the sprint cycle is adapted from *The Elements of Scrum* by Chris Sims & Hillary Louise Johnson, 2011, Dymaxicon, ISBN 10: 0982866917.

The authors write that “Scrum is a lightweight framework designed to help small, close-knit teams of people develop complex products. The brainchild of a handful of software engineers working together in the late 20th Century, scrum has gained the most traction in the technology sector … A scrum team typically consists of around seven people [7 +/- 2] who work together in short, sustainable bursts of activity called sprints, with plenty of time for review and reflection built in. One of the mantras of scrum is “inspect and adapt,” and scrum teams are characterized by an intense focus on continuous improvement— improvement— of their process, but also of the product.”

[Interested?]( https://www.cprime.com/resources/what-is-agile-what-is-scrum/)

A *sprint* is a development session (or *iteration* in standard Agile parlance), traditionally lasting anywhere from two weeks to a month (but never longer), during which time the steps in the agile iteration process (see Figure NA) are executed in one sequence from top to bottom. Scrum sprints are increasingly shorter now, many lasting only a week. The value in short sprints is that deliverable and value-creating software is output at the conclusion of each sprint, and adding value quickly is a good thing.

Scrum is simple in its organisation and recognises only three what they call “roles”, *viz:* Product owner, Scrim master and Team member. What’s the responsibility of each?

The *Product Owner* is responsible for maximising the ROI from the investment in the system. They do so by actively directing activities of the Scrum team towards ROI-enhancing activities and equally actively away from non-ROI-enhancing activities (such as much of the bureaucratic process inherent in the SDLC). They do so by controlling the *priority* of activities on the team’s *backlog* (more on this soon) and by ensuring that the team clearly understands the requirements – requirements than can and do evolve, change, morph and appear/disappear as the product matures. The owners accomplish this partly by recording the requirements in the form of what are called *user stories* in the form of “As a {role}, I want feature {a feature} so that I can {accomplish something}.”  

Such user stories are added to what is called the *product backlog* (discussed below) which might be construed as a type of requirements list in the sense of the SDLC, but are much less rigorous while at the same time much more targeted towards directly adding value to the system. 

In a nutshell, the role of the Product Owner can be summarised as follows: The Product Owner:

- Holds the vision for the product
- Represents the interests of the business
- Represents the customers
- Owns the product backlog
- Orders (prioritizes) the items in the product backlog
- Creates acceptance criteria for the backlog items
- Is available to answer team members’ questions

The next role to consider (although the authors of the book from which this brief introduction is taken actually outline the Scrum Master’s role next – I feel this is out of order and take responsibility for this shift in sequence) is the of *Team Member*.  The authors have this to say about the role:

“The role of each and every team member is to help the team deliver potentially shippable product in each sprint.” The *sprint* is best understood as one complete vertical sequence in Figure NA, from Requirements to Evaluation. This “sprint (or iteration) is designed to deliver a working system to the client, albeit mostly incomplete until the project nears completion. 

Microsoft, for example, has begun to use Agile to deliver its Visual Studio programming environment. 
[Interested?]( http://arstechnica.com/information-technology/2014/08/how-microsoft-dragged-its-development-practices-into-the-21st-century/) Make sure to read the whole article! 

The Scrum authors continue “Often, the best way for a team member to do this is by contributing work in their area of specialty. Other times, however, the team will need them to work outside their area of specialty in order to best move backlog items (aka user stories) from *in progress* to *done.*”

In a nutshell, the role of Team Member can be summarised as follows. The Team Member:

[Is] responsible for completing user stories to incrementally increase the value of the product 
Self-organizes to get all of the necessary work done 
Creates and owns the estimates 
Owns the “ how to do the work” decisions 
Avoids siloed “not my job” thinking

The final Scrum role to consider is that of Scrum Master. The authors write that “While a team’s deliverable is the product, a scrum master’s deliverable is a high-performing, self-organizing team. The scrum master is the team’s good shepherd, its champion, guardian, facilitator, and scrum expert.”

Further, they offer that “The scrum master is not— we repeat, not— the team’s boss. This is a peer position on the team, set apart by knowledge and responsibilities not rank.”

The role of the Scrum Master can be encapsulated as follows: The Scrum Master is a/an:

- Scrum expert and adviser 
- Coach 
- Impediment bulldozer [love this one :)]
- Facilitator

The Scrum uses various tools (called *Scrum Artifacts* by practitioners).  

### Scrum Artifacts

Artifacts (defined as: ”*any object made by human beings, especially with a view to subsequent use* (dictionary.com)) include the aforementioned Product Backlog, about which the authors write: “The product backlog is the cumulative list of desired deliverables for the product. This includes features, bug fixes, documentation changes, and anything else that might be meaningful and valuable to produce. Generically, they are all referred to as “backlog items.” While backlog item is technically correct, many scrum teams prefer the term “user story,” as it reminds us that we build products to satisfy our users’ needs.” 

The Scrum Master sorts the backlog in order of descending priority. The stuff at the top of the list gets done first. 

Furthermore, “Each item, or story, in the product backlog should include the following information: 

- Which users the story will benefit (who it is for); 
- A brief description of the desired functionality (what needs to be built);
- The reason that this story is valuable (why we should do it);
- An estimate as to how much work the story requires to implement;
- Acceptance criteria that will help us know when it has been implemented correctly.”

A second artifact is the *Sprint Backlog*. The authors write: “The sprint backlog is the team’s to do list for the sprint. Unlike the product backlog, it has a finite life-span: the length of the current sprint. It includes: all the stories that the team has committed to delivering this sprint and their associated tasks. Stories are deliverables, and can be thought of as units of value.  … Each story will normally require many tasks.”  

The sum of these tasks for a story can be considered the *scope* of the story – how much detail or how many processes are involved in the story.

A further Scrum artifact is the *Burn Chart*, which shows how much of the scope (tasks required per story) have been covered by the team over the specified period of time. A progress chart if you like. 

Next comes the *Task Board*, which is visible to all team members, the simplest form having just three columns: 1) To Do; 2) Doing; and 3) Done.  Elegant. Simple. Transparent. 

Finally (for our purposes) is the notion of *done*. It might seem simple to you or I. If something is done, it’s done. Not so in the world of development. There are conflicting realities and stages of *doneness*.  The authors write” “A programmer might call something done when the code has been written. The tester might think that done means that all of the tests have passed. The operations person might think that done means it’s been loaded onto the production servers. A business person may think that done means we can now sell it to customers, and it’s ready for them to use. This confusion about what “done” means can cause plenty of … trouble, when the salesperson asks why the team is still working on the same story that the programmer said was done two weeks ago! In order to avoid confusion, good scrum teams create their own definition of the word “done” when it is applied to a user story. They decide together what things will be complete before the team declares a story to be done.”

Scrum includes a great deal of communication with all affected parties in the software development process. A *sprint cycle* (akin to an iteration in Agile-speak) consists of the following meetings, often called *ceremonies* in Scrum-speak:

1.	Sprint planning 
2.	Daily scrum 
3.	Story time 
4.	Sprint review 
5.	Retrospective

Ceremonies, as they are all about communication, are at the heart of Scrum. 

[Interested?]( http://scrummethodology.com/scrum-meetings/)

To sum up, Scrum is a straightforward, lightweight method for building software where user requirements can change and useful, value-adding software needs to be produced quickly. Scrum is a collaborative method, focussed on continuous improvement of not only the end product (software) but also on the actual process of making the software. 
[Interested?]( http://scrumtrainingseries.com/Intro_to_Scrum/Intro_to_Scrum.htm)

Here’s a great summary of everything we’ve presented here, and more. It’s not along read, and it’s recommended to get the broadest overview of the various development methods along with recommendations on when to sue teach. Highly recommended.
[Interested?]( http://www.cms.gov/Research-Statistics-Data-and-Systems/CMS-Information-Technology/XLC/Downloads/SelectingDevelopmentApproach.pdf)

## Which to choose?
When it comes down to the choice of development methods between the traditional waterfall method and the more contemporary (but not by much) agile methods, the choice is often made for you by the nature of the problem or opportunity the system will address and by how well you can scope out the requirements in advance. Specifically, there are two broad categories or types of requirements: the *shall* and the *should*. This folds nicely into our discussion of *creep*. 

*Shall* requirements are *prescriptive*, as in *must be done*. When your physician writes a *prescription* for you, it's not to suggest that you might want to think about doing this or that. It's an order. Take this medication this number of times a day and with this food. Avoid this substance (alcohol usually) while on this medication. Do not double up if you miss a dose. And so on. These are strict rules to follow. These are *shall requirements*. 
*Should* requirements, on the other hand, are less *prescriptive* in nature and more *nice to have*. They are to be included in the system if things go well, or if they fit into the final model or if they can be afforded when all is said and done. These requirements are more likely to arise in the *process* of building the system as outcomes are discovered and contingencies are explored. These are the kind of requirements that prototyping and agile methods uncover.  

In general then, if you can absolutely positively spec each requirement in advance at a very granular level, then the waterfalling SDLC is for you. Here’s what to do, go do it. If things are more fluid, if the target is moving, if the real nature of the problem or challenge is evolving or is poorly understood, you’re better off with Agile. Discover as you go, but make sure you have a target and a resource cap else the process will be never-ending.

Here's an example of *prospective requirements* that I created so that a developer could code up the *dynamic auction system* that is used in my class. This is a small subset of the specs, but they are of the *please do this* nature. The developer and I aren't *discovering* anything:

--- *Begin specs* ---

Ruleset 4: Run Game (Scenario C)

Processes:

a.	Start game

	i.	Set gPending = False in tGame

	ii.	Deny Student login

	iii. Fetch and write Start time to tGame

	iv.	Calculate and write Total players to tGame

b.	Pause running game

	i.	Stop the timer

	ii.	Preserve all elements

	iii. Function to resume running game

c.	Running game (Client)

	i.	Poll and show game status (running = green / stopped = red)

	ii.	Poll and Enable BUY! button as appropriate

	iii. Show total game time in seconds

	iv.	Show time remaining (Start time + Duration – Current time) in seconds

	v.	Show Min buy number 

	vi.	Show Max buy number

	vii. Show already bought this client

	viii. Show average paid this client

	ix.	Show average paid all clients 

	x.	Fetch current price

	xi.	Show current price

d.	Create bid records in tBids

	i.	Fetch oID  from tLogin and write boID to tBids

	ii.	Fetch sCUID  and write bCUID to tBids

	iii. Fetch IP from running game 

	iv.	Fetch oIP form tLogin

	v. Compare IP with oIP

	vi.	If same write IP to tBids else exit

	vii. Get and write Player bid 

	viii. Get and write Time bid submitted to tBids

	ix.	Calculate total bids this player

	x.	If total <= Max bids then Enable bid button else Disable

e.	End game (duration override)

	i.	Get and write Elapsed time to tGame

	ii.	Get and write Success (game came to resolution) to tGame

	iii. Fetch and write Total bids entered to tGame

	iv.	For each CUID in tBids where oID = gID (current game)

		1.	Verify Sum of bids in tBids >= Min Bids and <= Max Bids

			a.	If within range

				i.	For each bBid in tBids

					1.	Increment counter

					2.	Store Bid price

				ii.	Next

			b.	End if

		c.	Calculate Average Bid price this student

	v.	Next 

	vi.	Get and write gBonus = if bonus marks awarded based on this game to tGame

	vii.	Sort Averages in descending order

	viii.	For each CUID in tBids where Average bid > 0

		1.	If Average >= gCutoff_1 and < gCutoff_2 from tGame

			a.	Increment gActual_1

			b.	If bBonus in tGame = True then

				i.	Create record in tBonus

				ii.	Write CUID from tBids

				iii.	Write gID from tGame

				iv.	Write nBonus = gBonus_1 from tGame

			c.	End if

		2.	End If

		3.	Else if  Average >= to gCutoff_2 from tGame

			a.	Increment gActual_2

			b.	If bBonus in tGame = True then 

			i.	Create record in tBonus

			ii.	Write CUID from tBids

			iii. Write gID from tGame

			iv.	Write nBonus = gBonus_2 from tGame

			c.	End If

		4.	End If

	ix.	Next

	x.	Write gActual_1 to tGame

	xi.	Write gActual_2 to tGame

--- *End specs* ---

Complementary software development methods to systems development life cycle are:

- Software prototyping
- Joint applications development (JAD)
- Rapid application development (RAD)
- Extreme programming (XP); extension of earlier work in Prototyping and RAD.
- Open-source development
- End-user development
- Object-oriented programming
 
[Source SDLC Wikipedia](http://en.wikipedia.org/wiki/Systems_development_life_cycle)

## Why do systems development projects fail?

An Oracle (originally a database vendor but now a full-service application provider including, as we have seen, in the ERP space) developer with many years of experience wrote a sponsored piece in the Harvard Business Review about information systems delivery. He began with, "Information systems projects frequently fail. Depending upon which academic study you read, the failure rate of large projects is reported as being between 50%-80%. Because of the natural human tendency to hide bad news, the real statistic may be even higher. This is a catastrophe. As an industry we are failing at our jobs." (Paul Dorsey, [*Top 10 reasons why systems development projects fail*](http://www.hks.harvard.edu/m-rcbg/ethiopia/Publications/Top%2010%20Reasons%20Why%20Systems%20Projects%20Fail.pdf) - accessed June 18, 2015). Add this to what we have already seen from McKinsey regarding failure rates [(take me there)](#fail) and ICT doesn't have the best track record for delivering systems as promised. 

First of all, how do we define *failure*? That's important. The vast majority of systems are actually delivered, but what causes them to be deemed a failure is how well they measure up to the following metrics:

1. Was the system delivered on time?
2. Was the system delivered at or under budget?
3. Does the system meet the agreed-upon systems requirements?
4. Does the system provide acceptable quality output?
5. Has the system delivered ROI over its predicted lifespan?

On-time and at-budget are the easiest to measure. There's a delivery date and there's a budget. It's black and white. I have no sympathy for either a developer or a client who cannot set and meet defensible time and resource goals. There's simply no excuse for missing such targets. As a developer, it's up to you to make sure you have a good set of requirements and have estimated the time and resources required to meet those targets. If you are slipping, get back to the table and work it out. Be above board, flexible and, above all, willing to swallow your pride and work harder to meet the promised deadlines. If you are unsure of the requirements and are going Agile to discover the core need, then that must be accounted in the time and resource scope. While *how long it will take to discover the requirements* might be tough to nail down, there must be a time limit on this phase. If you can't find your way in six months, let's say, then the project is cancelled. Fair enough. As a client, it's incumbent on you to keep the project scope in check, to not make unreasonable demands just because you think the developers will cave, and to have a critically sympathetic ear for changes and impediments and new developments that could never have been foreseen and are unavoidable. There will be challenges along the road. It's an awful lot like a marriage. Having been married for 40-some-odd years, I know what I'm talking about.

Criteria numbers 3 to 5 are the most problematic since they are not as easily measured. In general, these three are about *expectations* and expectations are notoriously slippery and malleable. I know. I've been on both sides of the developer/client relationship. I've developed (architected, designed, coded and delivered) many, many systems, large and small, in teams ranging from myself alone to multiple specialists. And I've worked as a solutions architect and senior consultant, abstracted several layers from both the coders and the clients. And I've also been a client for many systems. I currently have three systems under development. I've been a senior developer and a senior manager. I've worn all the hats. 

So yes, expectations are difficult to manage. Requirements are difficult to nail down, especially when clients *don't know what they want until they see what they DON'T want..."* (this is a real saying in the systems development world). Sad, but all too often, true. Clients come and go. Client expectations change. Senior management shuffles responsibilities. Your team memberships will change. Technology evolves at an alarming pace. New people need to be *re-sold* on the virtues of the system. Budgets dry up or, maybe worse, budgets get pumped full of cash at fiscal year end and must be spent (talk about needlessly raising the bar on expectations). Business requirements (the context) change. Everything is in constant flux, so meeting expectations is like trying to hit a target that's moving multiple times faster than your ability to adjust your sights. So in many ways, it's surprising that *any* system gets delivered and considered a success. 

That being said (and I feel better for having said it and defending my old friends in the business), there are some well accepted ground rules for developing systems that are timely, efficient, well received by users, that bring value and are a delight to use. They are:

1. The proposed system must be doable. So many cascading effects will kick in once you agree on a project and a delivery date. If you fail to deliver, anything that depends on your project being in place is put in jeopardy as well. Not a good way to make friends around the organisation. 
2. Strong governance (the rules about how the rules are applied) is a must. 
2. There must be a real, defensible business case for the system and clear ROI (return on investment) targets and time lines. When will the system start paying us back? What's the *time to value*? Not every (or even *many*) system will pay back starting on delivery day +1. Everyone must agree on this expectation.
2. System requirements must be clear to all concerned parties - everyone must know what's in, what's out and what's expected from the system. *Scope creep* must be managed. If things change, requirements and deadlines and allocations must be revisited and time milestones, resources or both need to be adjusted. Clarity here is paramount. Protect your project scope as if it were the very oxygen you breath for life.
3. Projects must be carefully costed and budgeted. Skill in this area is critical. Estimates must take into account the expected project deliverables, carefully overlaid on the chosen systems development methodology. Agile methods, while they appear to have more fluid boundaries and more liberal expectations, are still measurable and predictable. The project estimation staff must be crystal clear on requirements and on how long it will take and with what resources to meet those expectations in order to accomplish the tasks given the chosen development method.
4. Senior management buy-in and a champion is required if you're working in a large organisation. Someone near the top needs to love the system and be willing to go to bat for it. It's a given that times will get touch on large projects. You need a champion who will fight for your vision and keep the excitement going even in the tough times.
5. You need a good team, and especially (but not exclusively) project managers. PMs will manage expectations, and expectations are your biggest challenge. 
6. It's critical to have great technical people - people with vision,imagination and a willingness to make things work. It's hard to keep good techies. There is incredible demand for talented people who can make technology sing. 
7. You need good tools. There is absolutely no substitute for having the right tools available to do the job.
8. Don't cut corners to compensate for a tight time line or a lack of resources. Get back to the table with the client if things are going off the rails. A good project manager will ensure this works.
9. Don't get seduced by cool, new, cutting-edge, fancy technology. It could be here today and gone tomorrow. Make sure all the pieces fit together. Make sure the project can deliver on expectations. Don't do something because you *can*. Do it because it adds value.
10. Get end-user buy-in and acceptance early and maintain as close and as good a relationship with those who will ultimately use your system as possible. Many shiny new systems have failed on the shop floor when users refused to use them, once delivered. Prototyping with end-users is a great way to earn user respect and acceptance. And it reduces the need for training as well.    

##Interface design and programming

![Now that's efficient!](https://raw.githubusercontent.com/robertriordan/2400/master/Images/chuckle_bros_efficiency.gif)

[Source: <http://www.gocomics.com/chucklebros>]

There are two *interfaces* to which are alluded in the cartoon above (which is brilliant from an entrepreneurial perspective). An interface is the place where two systems with different *immediate* goals interact and fulfil the goals of a *higher-order* system. It's where systems talk to each other and exchange stuff. 

So an *interface* is a *meeting place*. In the case of the above cartoon,it's *people meets beer* and *people meets... well, urinal*. It's commerce. To sell beer efficiently at this client-server interface, there needs to be two systems simultaneously available and functioning. Input and output. Even if you've never been to a bar, surely you've been to a McDonald's. Imaging those taps in the cartoon full of pop or tea or coffee or even water. There would need to be a way to deal with the output in order make room for new input. *Ergo* there are two interfaces here. The interface to the input and the interface to the output. It is to the *design* of these spaces where humans meet machines, the *user interface*, that we now turn our attention.

[Interested?](http://dictionary.reference.com/browse/interface)

I fancy myself an interface designer. If there was anything about creating systems that I enjoyed (other than a really tight algorithm that churned out accurate results in the blink of an eye), it was the art of the beautiful interface. And beauty, in an interface, has less to do with nice fonts and complementary colour schemes (although that's important too, don't get me wrong) than to the raw simplicity that that allows people to just attack and use it and feel as if they were smart because it was so obvious to them how to make it sing. They could make music with the interface. It was a *Stradivarius*... something that in the right hands could create beauty itself. OK that's just a bit overstated, but the feeling is genuine. It's nice to see your interface being used by people who just *knew what to do with it* and could use it in the way it was intended and it just worked seamlessly. It's so nice watching people feel smart. It's a bit like designing a piece of clothing and seeing people wearing it in the wild. So satisfying.

Interface design and algorithm design go hand in hand. From the interface perspective, people need to be able to address it and instinctively know what to do with it. It has to be what we call *intuitive*. It must suggest to the user how it can be used. Much of this usability, which the [Business Dictionary](http://www.businessdictionary.com/definition/usability.html "Usability definition") (on June 20, 2015) defined as: "Ease, speed, and intuitiveness in operating or using a device, service, or facility. Usability arises from a combination of well thought-out architectural and design factors, and translates into user's ability to successfully perform tasks and solve problems with customary effort." Pithy, and right on the money. So how to get there?

Much of what we consider to be the interface in a piece of software has been standardised by the operating systems we have grown accustomed to using. The various widgets and icons we use have come to accepted ways of communicating choice and receiving output. The general term for our ability to understand what these tools is encapsulated in the term *affordance*, defined by dictionary.com (on June 20, 2015) as: "A visual clue to the function of an object."

Take a button as an example. A everyday, garden-variety interface button, as below.  

As savvy and experienced computer users, we know what to do with a button. When we click them with a mouse of tap them with our finger we effect some change. Our *input* by clicking will set some *process* in motion, resulting in some *output*. It all goes back to the simple system we've talked about since Chapter 1. But what is it about the button that cues us to what to do with it? Why does a button *afford* clicking? 

|![button](https://raw.githubusercontent.com/robertriordan/2400/master/Images/button_no_caption.png)

First and foremost, it affords clicking because we have learned that objects that look like buttons operate by clicking. Elevator buttons. Microwaves. Telephones. Keyboards. Radios (at least in the old days anyway). So *graphical user interface* (GUI) designers leveraged everyday things that people already knew how to operate and built them into the human-computer interface. The early designers of what we now know as Windows and Mac interfaces pioneered these techniques (and the use of a mouse as an input device) in the 1970s at Xerox (the photocopier people) Palo Alto Research Center (or PARC). Xerox PARC is legendary in computer geek history. They are responsible for some incredible, ground-breaking advances in computing including, but not limited to, laser printing (obvious when you think about their signature copier machines), ubiquitous computing (computing everywhere with multiple devices talking to multiple devices through middleware - this is the origin of the *internet of things* which we will discuss at length elsewhere in this text), Ethernet (the wired foundation of local area networks), object-oriented programming (which also makes sense in terms of their pioneering work in GUI), the concept of the computer desktop and to top it all off, no less than the origin of the personal computer that we all know and love.  

[Interested?](https://en.wikipedia.org/wiki/PARC_(company))

So the nice folks at PARC used everyday objects and then created what we refer to as *idioms* for other functionality. Things that have real-world correspondence like *check boxes* for multiple item selection, *radio buttons* for single selection (like an old-time radio where only one button could be depressed at a time, the already selected one popping back out when you pressed a new one to select a different channel), an *image container* like a picture frame and a matched set of *scroll bars* (one horizontal and one vertical) and a brand-new idiom, the *dropdown* menu. Here are some examples:

|:--------:|:------------:|
| Button with visual cue | Check boxes for multiple selection 
 | ![button](https://raw.githubusercontent.com/robertriordan/2400/master/Images/button.png) | ![check box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/check_boxes.png) 
| Series of combo boxes | Image container and scroll bar 
| ![combo box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/combo_box.png) | ![image + scroll bar](https://raw.githubusercontent.com/robertriordan/2400/master/Images/image_scroll.png) 
| Radio buttons for single selection | Label and text box for data input 
| ![radio button](https://raw.githubusercontent.com/robertriordan/2400/master/Images/radio_buttons.png) | ![text box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/label_text_box.png) 

But what really got the ball rolling was the idea of the desktop, and virtual windows onto that desktop. Again, Xerox PARC were the innovators who brought these idioms to the world. (Many Apple devotees erroneously believe that it was Apple which 'invented' Windows and the GUI and icons and mouse input etc. Not so. Everyone followed PARC's lead. They were the first.)   

To go with these idioms and metaphors that allow us to instinctively understand how to interface with a machine (my 93 yer old mother is a whiz with an iPad and the envy of her nursing home residents and the much younger staff), designers have used all manner of both traditional and new research and best practice to build intuitive interfaces. A large part of the miniaturisation we have experienced is the result of clever work with icons, which have largely been standardised across vendors and platforms and have allowed for complicated symbols to be communicated in 15 X 15 pixel format. Some examples are below. Give yourself a little test. If you can't associate an icon with an activity or command or an idea, punish yourself with two hours of aimlessly bashing your favourite device:

![text box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/icons.png)

Combined with traditional knowledge about visual design (an enormously interesting and exciting field) such as Gestalt (for closure and similarity and proximity [Interested?](https://www.google.ca/search?q=gestalt&biw=1920&bih=950&tbm=isch&tbo=u&source=univ&sa=X&ei=HNqFVbDmKIuMyASU8oL4Cw&ved=0CE4QsAQ#tbm=isch&q=gestalt+figure+ground) and cognitive psychology (another course you should take at least one of in your career), rich and effective interfaces have evolved into use and are accepted means of communicating with machines. In this I am reminded of French aviator and author *Antoine de Saint-Exupéry* (1900-1944 – author of The Little Prince), who characterised engineering elegance by writing: "A designer knows when he has achieved perfection not when there is nothing left to add, but when there is nothing left to take away." So like any other piece of engineering, it’s not about a *tour de force* but rather about simple elegance. Edward Tufte (the godfather of modern information visualisation) included the following line in his dog and pony show (which I attended in 2010 in San Francisco) "No matter how beautiful your interface, it would be better if there was less of it." Wise words. Simple elegance is the designer's goal. 

The desktop metaphor and the idea of iconic representation of tasks and the manipulation of things in the interface (such as files and folders and removable drives such as a USB) would not be workable in the absence of another of PARC's great initiatives: *Object-oriented Programming* (OOP). OOP describes *objects* (everything is an object including command buttons, file folders, printer icons, and all the way down to a single cell in a spreadsheet application) which can have *properties* (characteristics that describe them - for example a simple OK button has a width, height, position in the window in which it appears, a caption (what it says - such as 'OK already!' or 'Print') and a background colour, font, etc.), *events* (things that can happen to it over its lifespan - for a button, the most important is the *click* event) and finally and most importantly, *methods* or things it can accomplish given the occurrence of an event. Designers design the interface, set the properties and scope the events to which the object should respond. Programmers write the code that executes when an event occurs to the object. This genre of programming is referred to as object-oriented, event-driven programming. [Interested in OOP?](http://searchsoa.techtarget.com/definition/object-oriented-programming) 

Figure RS. Object-oriented, event-driven programming example

![application](https://raw.githubusercontent.com/robertriordan/2400/master/Images/code.png)

Here we have a little window with a purpose: to display and sort the names of TAs for a course. At the very top, we find a window label indicating what we are looking at. "My TAs". How nice (and they were all great people who worked very hard for me). The blue strip at the top also includes other *objects* owned by the window, such as the *minimise, maximise* and *close* functions on the right of the strip. Below that, the meat of the interface - the actual list of TAs (not yet sorted at all, just dumped into the list box, which is an object that holds lists of things). Below that, we see two command buttons, one to sort the list in ascending order (from A to Z) and one to sort in the opposite direction. Below that, we have two buttons to deal with disposal of the window; one to cancel the operation (likely the list would be returned to it's sort order on entry to the window, but that would have to be coded) and one to close the window and return control to the calling window (the greyed-out window to the right with the little "Show TAs" button that actually asked for this TA window to be shown). 

Let's further unpack this figure. Each of the objects, including the window itself, has Properties that can be set by either the interface designer or the coder. These properties can be set once and left alone, or can be controlled by code. Objects can be made to appear and disappear, for example, depending on the context of the application. Each object here has properties such as 'Top' indicating the location of its horizontal position (or top, left corner) in the window container. Analogous properties include 'Left', indicating its vertical position (how far left in the window), its width and height and caption (what the button's label says). Note the 'Cancel' button has a red caption (and is a different size) to set it off from the other exit button. 

There are plenty of design principles on display here. The first thing your eye would catch on displaying this window is the reasonably discrete 'My TAs' window caption at the top, serving as a reminder of what the window displays. The actual list itself is the most prominent object on the interface (through the use of a coloured background) as this is the real purpose of the window, and our eye is drawn there next. Once we are satisfied that we are looking at what we intended, we next see the actions or work we might want to accomplish with our list. Finally, we see the disposal actions, with the only *emergency* action indicated in red, and the most likely disposal action *Close* at the very bottom, right of the window. This is where people look for actions when they are done.

There are also other more subtle things going on. Gestalt things (remember Gestalt?). Specifically, grouping through proximity, size and colour. Note the space between the two sort buttons is smaller than the space between the list box and the first command button. Note that the space between the list box and the first command button is the same as between the bottom sort button and the top of the disposal buttons. So we have three functional groupings here, defined by proximity, to represent the three function types on this interface. It's a subtle confirmation (and our brains are aware of this whether we know it or not) that there are three things going on here. Finally, size (and colour) is used to set off the 'Emergency' possibility. Our eye is drawn to the differences between this and the other objects in the window, and that's a good thing. We want our user to have a good exit strategy. Note that *Emergency Exit* signs in public buildings are always lit and always red. We borrow that motif here on the interface.  

While this example barely scratches the outer shell of the discipline of design, it does illustrate some things of which users are not consciously aware, but which are enforced for our (user's) good. When an interface is designed with simplicity in mind, and *from the perspective of the user* and not that of the designer, we, the users, use it and feel smart about it. That's an important goal of design. Make the user feel smart which, by the way,  makes for a smart designer.

Let's finally look at the code, which is why we're *really* here. I have written the code in an odd, hybrid pseudo-code / real code format in an effort to make it understandable. If it's not, I'm sure you will let me know. But let's walk through the logic of the program flow here. 

First, this window is 'called' from another window, through the user clicking the "Show TAs" button on that interface. The TA window will then appear (pop up) over the calling window and will have 'the focus' - meaning that the all the user's actions will be directed to this window because it is 'in focus'. We might argue that this is an overly simplistic example, and in fact the button on the calling interface should be "Manage TAs" and there should be functions on our TA interface to add, delete, edit, pay, schedule and all the things a professor would need to do with their TA objects over the life of delivering a course. And this is true. But let's just digest this simple example here, with a few simple functions, before being critical of the overall design.

So our TA window now has 'the focus.' We can observe the list of TAs and exit (through either Cancel or Close), or we can choose to sort our list. Knowing that the sorting of the list in either ascending or descending order is a *requirement* of the system, the designer and coder (programmer) would work in tandem to ensure that this functionality exists in the system. The designer would have placed the buttons in the appropriate places on the interface, with the appropriate properties set, and then handed it over to the coder, whose job it is to write the code to do the work. Since the most obvious thing to do with a button, as a user, is to click it (buttons *afford* clicking after all), the coder would place the code to do the sorting in the 'Click' event of the button. The interface just sits there being all *interfacey* until the user *does something* such as click a button. Those buttons have other events to which they can respond as well. For example, the 'MouseOver' event, which is fired when the user passes the mouse over the real estate occupied by the button. The programmer might stick some code in the 'MouseOver' event to pop up a little message such as "Click me!' when the mouse in over the button. Likely not, but you get the idea. Here, the most important thing is to react to the *Click* event of the button and do some work since that's what the user, who knows all about how to make an application sing, expects.. So in the space in the program (we are skipping over massive detail here by the way) reserved for reacting to the Click event of the 'Sort ascending' button, we have the code in green in the Figure.

So let's take a deep breath and dive in. When the button is clicked, the code associated with the event is executed sequentially, from top to bottom. We first ask the machine to count the number of items (TAs) that are in our list box. We have nine. We then stick that number into a *variable* - an area in memory that can hold onto a value for us and to which we can later refer when we need to use it. So we've got a variable named 'z' which now holds the number 9. We are using a (simplified) Bubble Sort algorithm here to order the items in our list. We next launch into that algorithm and tell the machine to iterate (step) through each item in our list box and compare it with the item *following* it in our list to determine which is *greater*. This is effected implicitly by the operating system of the machine by examining the ASCII codes of each letter in the TA name beginning with the first letter of the last name. If you forget what ASCII codes are, please go back and review in Chapter 1. So to begin, the machine would compare 'Jamieson' with 'Shapransky', starting by comparing 'J' (ASCII code 74) with 'S' (ASCII code 83). Since 74 is not greater than 83, these two items are already properly sorted and the algorithm moves on to compare 'Jamieson' with 'Platt'. Same deal. Next comes the comparison with 'Ahmed'. Oh oh. Trouble in Toyland! 'J' is greater than 'A' so we need to swap and move Ms. Ahmed up a notch (or demote Ms. Jamieson, whichever way you look at it, it's the same). The process continues until all items are sorted in ascending order, the sorted list is displayed again (which occurs automatically thanks to the operating system). The interface then just interfaces along, waiting for the user to do something else.

Note that appropriate code would need to be written in the other button's *Click* events as well in order for them to function as expected. 

And that's the thousand kilometre high view of interface design, objects, programming and systems. There's an incredible world of exciting work out there in design and programming. Don't discount it until you consider how satisfying it can be to create systems that make people feel smart. I still love getting my hands dirty in low-level design and coding. I can get lost for days in it. 

[Interested in User Experience Design?](https://en.wikipedia.org/wiki/User_experience_design)

[How about User-centred Design?](https://en.wikipedia.org/wiki/User-centered_design)

[Or simply User Experience?](https://en.wikipedia.org/wiki/User_experience)

[Or the underlying aspects of Human-computer Interaction?](https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction)


### Database design
Rabble babble and bump. TK.

See dropbox/BUSI 2400/ 2014_labs

A *database* is a collection of related data, organized in a logical order and used in many everyday activities. They are called *databases* because they store *data* and not *information*. Examples include a dictionary, a catalogue, TV listings and even the directory in a building, listing all the names of the people, perhaps their title and/or function along with the floor they occupy and their office number. Dictionaries, like phone books and building directories, are typically organized alphabetically. These are all non-electronic databases. 

Computerized databases are stored on electronic media either local to the user or in any of the various manners in which we store data remotely. They serve the same purpose as non-electronic databases. Examples include an MP3 music library, the contact list on your cell phone or in your email client.  

#### Why use a database?
Databases allow large (even massive) scale data storage. They also reduce possible errors (because they allow rules to be enforced about how things are stored and what *type* of data can be stored in the base). And if properly designed, they reduce *storage redundancy*, or the necessity to store the same piece of data in multiple locations in the database. The software provides fast, easy and efficient retrieval, viewing, searching, sorting, adding, deleting and editing of the stored data. Databases are used by almost all organizations in this information age.

#### Why not just use Excel?
What’s the difference between a spreadsheet and a database anyway? Aren't we just splitting hairs here? They both do the same things... right? Nope. 
Spreadsheets are optimised for a different purpose, and provide a simple flat file data storage system. They are optimised for doing calculations, for *visually* arranging data and providing *visualisations* (charts, graphs, etc.). Databases, on the other hand, are optimised for efficient and effective data storage and retrieval. We are not even given the courtesy of seeing exactly how the data are stored by the database. We see a representation of how the data are stored, but we needn't worry about those low-level implementation details. The *relational* functionality of databases is the key to reducing error and redundancy. We will return to this shortly. 

##### What is MS Access?
Microsoft Access is perhaps the most prevalent Database Management System (DBMS) on the planet, owing to the ubiquity of Microsoft Office. Access (and other databases) allows users to:

- Easily create and manage a database
- View information stored in the DB
- Search/Sort/Filter the information
- Insert/Delete/Update new and existing information
- Allow for a full *CRUD* (Create, Read, Update ans Delete capability for all data in the database)

Access is widely used by office and home users due to its user friendliness and the fact that it requires minimal technical skill. YOu can be up and working in a database in a few hours. 

#####How does MS Access work?
Access is a collection of objects (which we discussed above in terms of OOP). These objects include:

- Tables
- Queries
- Forms
- Reports

Data is stored within table objects, and queries and forms are used to retrieve data from those tables.

##### Key term summary

- Database: is a collection of related data
- Object: is a table, query, form, etc. within a database
- Table: is a collection of related data organized in fields (columns) and records (rows) on a datasheet

A table in a database corresponds to an *entity*. An entity is a person, place, thing or event of interest. Entities become tables, and store only things that are of the same nature. So a *person entity* would store only attributes (characteristics) of people. You would find nothing in a *person entity* describing a movie or a school, for example. Choosing the entities you will store in your database for a particular system is an important early step in creating a defensible *data model*. So the first question you need to ask when creating a database is "What are the entities about which I will need to store information?" What are the *things* that form my problem space? Do I have people? Do I have locations? Am I tracking events? Are things like trucks or books or coffee cups involved? It's important to understand that a single database solution can have all these things in the same database, in different tables, if they interact with each other. For example a *people table* can store information about persons and those persons might attend a series of events and have parking spaces and own a bunch of pieces of art. If the problem space includes all these things and they interact with each other then a database can store and manage all of them. But let's not get too far ahead of ourselves here.    

Tables are the basic structure in which data is stored within a database. They consist of a collection of records (one per instance of the entity - like one per *person* or one per *geological specimen* or per *real estate transaction*) and the records themselves are a collection of fields (which are characteristics of the entity and can vary from one instance of the entity to the next). So in a *person entity*, fields (characteristics) might be the person's name and age and perhaps email address or Twitter handle. So each record, in this case, would be unique because while people might have the same name and age, they cannot have the same email address or Twitter handle. This introduces an important concept - the Primary Key (PK) for a table. 

Every table in a database needs a PK. A Primary Key is a unique identifier for each record. Databases need a way to be able to retrieve a unique record for each entity, otherwise, we would never know whose record we were looking at if there were 15 people named Jane Smith or Mohamed Mohamed in our database. You are already aware of several unique identifiers in your life... such as your student ID, driver's licence number, and if you are a citizen, a newcomer to or a temporary resident of Canada then you have a SIN card with a unique 9-digit number. There are others I'm sure you know (Health Card for example). This is enforced so that there is never any doubt about to whom particular records refer. A database is no different. It must be able to unequivocally pinpoint the correct instance of an entity. Get the right person every time. 

So, one of the fields in every table is dedicated to a Primary Key. We will come back to Primary Keys and its attractive cousin, the *Foreign Key*, just below.   


Tables can be related to each other through Foreign Keys (FK)


Figure STT. Tables (Datasheet View)

![Design](https://raw.githubusercontent.com/robertriordan/2400/master/Images/db_design.png)

#### Terminology

*Field* is a column on a datasheet; it allows data of a particular type to be stored in records in a table. In Microsoft Access, a field has a Name, an associated data type and a Description. This data, associated with the fields in a table, are referred to as *metadata*, literally *data about data*. In Figure STT above, we see that the table named *tStudent* has four fields, *sCUID*, *sTerm*, *sYear* and *sSection.* You can see their data types and the description of the field's contents in the Figure. Fields are sometimes referred to as 'attributes', which is a synonym for *characteristics.* 

Figure STP. Tables (Design View)

![Datasheet](https://raw.githubusercontent.com/robertriordan/2400/master/Images/db_datasheet.png)

*Record* is a row on a datasheet and is a collection of values defined by fields. In our auction application, the four fields are shown here *populated* (data entered) for three records, corresponding to three students. The first record captures the information for a student with the CUID of '100999997' (in a field names sCUID - the 's' prepended indicating that it's in the Student table), a Term indicating 'Fall', in the Year 2015, and who is, finally,in Section 'A'. Two other students have been entered below. This is from a real, live, database. This is what it looks like.  

##### Primary Keys (PKs) and Foreign Keys (FKs) 

Now that we have a basic understanding of tables (entities), fields (attributes) and records, we can move on to the more sophisticated consideration of what makes *relational* database management systems so powerful. Let's imagine trying to implement the auction application in Excel, for example. We will just consider (model) the Student/Bid part of the application. Some background is necessary here.

The *auction application* is a tool that I use in my Intro to ICT class. It pits student against student in a real-time race to buy a required number of some commodity item in a restricted amount of time from a dynamic system that accepts *bids* in the form of a commitment to purchase. A *bid* in this game is simply a *sale*. In *real speak*, what might happen in a hypothetical game is that all students in the class are required to virtually purchase between 7 and 10 items (it matters not what the items are) in a 5 minute period. There are not enough items in inventory for every student to purchase 10 items. The group of students (half the class, or 25% of the class, it's up to the instructor) with the lowest *average* purchase price for their purchased commodities is awarded bonus marks. The trick is that as the *velocity* of purchasing by the group increases, the higher the price rises. In order to fulfil the requirements of buying a minimum number of items at the *lowest* average price, it's necessary to buy when no one else is buying, because the price is dropping. But the clock is ticking... anyway, it's a fun simulation and it's meant to stimulate thinking around disrupting how commodity items are sold in public venues (such as concerts or sporting events). That's enough detail for our purposes here.

Let's now consider how much data needs to be tracked in this simulation if our goal were *only* to award bonus marks. We'd need the student ID and the exact price at which the article was bought. And seriously, that's it. But if we wanted to control the *context* in which the bids happened and to be safe and secure and to be able to enforce the rules around the process so that if challenged, we could say with near certainty that Student X bought Y articles at this array of prices and that the competition was still open at the time of final purchase and that any particular *sale* occurred while the simulation was running and that there was no data entry error, and that the student was actually in the classroom and participating etc. then we'd need a lot more information than just those two pieces of data. This is an important aspect of systems development. Often systems are created in order to provide *accountability* for processes. Systems, with auditable rules and archived inputs and outputs, are seen to be impartial appraisers of process. 

Imagine a spreadsheet storing the necessary data to allow for the distribution of bonus marks - marks which might mean the difference between a pass and a fail in the class. The stakes are high. So how to gather the data? One solution might be to insist that each student keep accurate records of their own transactions. There are *at least* two problems here: 1) Record keeping in the heat of competition might not be accurate, and, perhaps more importantly, doing so would almost certainly distract the student from their primary task - to buy stuff cheap. So that's not a great solution. We might also consider assigning a non-partisan, independent recorder to each student. Not feasible at all - who would it be? 2) Perhaps even more importantly, and here's where the audit issue comes into play, people are not always honest. Sorry to be blunt but after 35 or so years in the teaching game, I can state unequivocally that I don't expect *all* people to be objective assessors of their own performance, especially when something as critical as grades are concerned. Be honest with yourself. Would you trust *everyone* in the class to be capable of not only protecting their own interests, but yours as  well? So we are well advised to create a system which can *prove* it is impartial because, well, it's a machine and if the rules are properly encoded, there's no reason to suspect that bias is creeping into the system. It might be *systematically* biased (see precision in Chapter 2) but the bias will be equally distributed across players and not in favour of, or detrimental to, any particular player. That's how we *level the playing field*. Machines are impartial, don't care about your previous record or your current trajectory, don't lie, cheat, fall in love or get a mad on, and can't be easily fooled on the rules. And they provide some security if properly implemented.

We often defer to systems when challenged on whether or not rules or procedure were properly followed. And that's not such a bad thing. In fact, it's the essence of audit; a set of articulated, transparent and defensible rules, accompanied by evidence that the rules were followed. In the case of the system under consideration here, it's not feasible to collect data in any other way than centrally, through an application and a database backend. (A *backend* is a system component that is not visible to a user but which performs vital services for the process piece and for the *frontend* - the part which the user sees.) So all the transactions in this auction system are recorded in a database. But how to accomplish this efficiently? 

No surprise here but we need to create the *context* around the system in order for it to operate in a defensible way. We need all the rich soup of data that surrounds the simple system transactions (student bought something at this price) in order for the rules to be unequivocally interpreted and for the decision to be make with certainty. Thus we arrive at the data model in Figure EZ below.

Figure EZ. The auction system ERD (Entity Relationship Diagram)

![text box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/auction_erd.png)

While it seems daunting at first glance, it's not really that bad. Each box is a table. Recall that tables store *entities* which are *things*. So we have a *Bonus* entity (my convention is to prepend a 't' to each table name in order to keep objects clearly identified), a *Student* entity, a *Bids* entity and so forth. With the exception of the *Defaults* entity, each of the other tables has at least one odd-looking and more oddly-labelled line attaching it to another entity. These lines represent *relationships*, which provide some of the real value in database management systems (DBMS). These relationships between tables allow us to model several different kinds of relations between entities. The most widely modelled type is the *one-to-many*. This relationship is easily understood as *one* instance of an entity in a table can have *many* related instances in another table. So *one* student in a student table can have *many* courses in a courses table *without having to duplicate all the student data in the courses table*. This is the critical piece. Let's dig in. 

In the auction system modelled in Figure EZ, take note of the relationship between the *student* and the *bids* entities. At the student end of the joining line, we find a tiny number '1'. At the bid end, we find the symbol for *infinity*. This represents a one-to-many relationship between student and bid; literally, any *one* student can make infinitely *many* bids. While not exactly obvious, the relationship is forged between the student ID (in this case named sCUID - the prepended 's' referring to fact that it is in the Student table) and a field in the bids table named 'bCUID' - the 'b' because it's in the Bids table. Let's take this slow... this is the heart of the system. Recall we said that, in reality, all we need to administer this bonus marks scam is a record of the student bids. There would be plenty of other administrative tasks to complete around tallying the number of bids and making sure the rules were followed and creating the distributions to determine who would be awarded bonus marks, but the *core* was just those two things: student ID and bid price. This relationship between student and bid automates that requirement, but significantly reduces the data collection burden to achieve the same result. And this, we know, satisfies our input/output maxim for progress. We get a better outcome with less input, which equals *double progress!* So we have only one record per student, with one of the fields being CUID and, for each purchase (bid) made by that student, we replicate the CUID in the bid table. *Don't leave yet!* We're almost home... So one student can make many purchases but we only need to record that student's *Term, Year* and *Section* once, in the Student table.  

This needn't get too technical, and indeed there are plenty of reasons to use a database *backend* on this system that we don't have need to explain. But let's just imagine this system being run using a spreadsheet as the backend. And make no mistake, it could well be done using a spreadsheet, but everything would need to be specifically coded and automated and each instance of a game in class would need to be on its own *datasheet* and eventually there would need to be a new spreadsheet because we would exceed the 255 (open to interpretations and available memory) datasheet capacity of a single spreadsheet and there would be much manual thrashing about looking for a particular student in a particular game in a particular term and heaven forbid that student should drop the course and come back in a subsequent term or even switch sections mid-term. This would cause no end of headaches. A new auction would require the duplication of an existing simulation onto a new sheet and any number of manual adjustments including deleting all the bid data from the previous game in order to run a new one. A database can handle all such *exceptions* with ease, can store all the data for an infinite number of games in the same database and can quickly and efficiently retrieve any set of records according to any criteria in an instant using *Structured Query Language* (SQL - see below). Databases make my job, as administrator of the system, infidelity less complicated. And isn't that what we are after? Same (or better) output from same (or less) input. Indeed.

Finally, much of he data collected by the system are dedicated to making certain that we are accountable for the awarding of bonus marks. I need to be 100% certain that everyone who deserves a bonus receives it, and equally that no one who did not earn it, receives it. Tall order, but handled nicely by system design, coding and database design. 

#### Structured Query Language (SQL) - the 1,000km view

SQL is referred to as a *fourth generation language*. As the name implies, SQL is a rules-based language that, despite many slight variations, is the engine behind all relational database management systems.  

SELECT * (or other) FROM table WHERE ...

Then talk about automating that in a web app. For i = 1 to num_students

	Do some stuff
	
Next. 


#### UML, Use Cases and Use Case Diagrams - a light intro

Agile versus RUP

http://www.agilemodeling.com/essays/agileModelingRUP.htm

The *Unified Process* (UP) or the *Rational* Unified Process (RUP) is an *object-oriented* (OO) systems development process developed by the three *godfathers* of the OO approach, the famous Booch, Rumbaugh and Jacobson. Their pioneering enterprise was named Rational Software, which was subsequently bought by IBM.  UP is tailored to organisational and project needs, meaning is is less *presdictive* than SDLC and more on the *adaptive* side, *a la* Agile. It is use-case driven, using a design tool known as *Unified Modelling Language* or UML.

Let's interject here in the Scrum process and backtrack to the origin of the requirements that are being coded by the Scrum teams. What are these things that are being prioritised? They are *user requirements* (UR) and are the result of an intense exercise to elicit exactly what the system is meant to accomplish. Designers, architects and programmers need at least a general idea of what the system is meant to do, even if the exact details will emerge through an agile process of discovery. Unlike an ancient explorer who just *kept heading west* to see what they could find, our modern-day explorers need a good compass heading. We need to know exactly where we are meant to end up, even if the exact route is unknown. Such is the goal of user requirement discovery. 

The process is not exactly set in stone and can itself proceed along a path of discovery, but in the end, certain things must have been documented and agreed upon. They are the *Business Requirements* often shortened to BR# where # refers to a sequential number that can later be referenced when particular *Functional Requirements* (FR) are listed. Each FR must map back to a BR. The FR are the top-level things that the system must allow and accomplish. In our auction system, there are certain things that absolutely need to be accomplished. Examples are:

- BR1: Professors must be able to log into the system
- BR2: Students must be able to log into the system
- BR3: Students must be able to buy products
- BR4: Sales of products must be recorded along with other contextual variables
- BR5: The system must be able to decide who gets bonus marks and who doesn't
- BRn ...

There are often many others.

There needs to be a comprehensive list of such requirements. Note that *how* those things would be accomplished or *what* the interface would look like are not a priority at this stage. These are simply the highest level requirements possible. after assumptions such as "students must have a wi-fi enabled device", and "the classroom must allow wi-fi authentication" and "the internet must be available." Such things are often assumed, but can be stated in an even higher-level document, or as the very highest level precursors to a successful system in the requirements document. Sometimes, such high-level requirements are labelled *System Assumptions* (SA) and are listed and numbered in the same way as BRs.

Once all RAs and BRs are listed, the Scrum team *Product Owner* has the responsibility to prioritise the requirements. Let's refresh some of the PO's responsibilities. The Product Owner:

- Represents the customers
- Owns the product backlog
- **Orders (prioritizes) the items in the product backlog**
- **Creates acceptance criteria for the backlog items**

I've added the **bold** where the requirements document directly affects the roles of the Scrum teams. So what does that look like? Figure TM shows a mock-up of a *Use Case* in a *Requirements Document* that a Scrum team might use to help prioritise, create the system and remain accountable. There could be literally hundreds (or even thousands depending on the size of the system) or these to prioritised in the Scrum:

Figure TM. Use Case sample

![Use case](https://raw.githubusercontent.com/robertriordan/2400/master/Images/use_case.png)

For Diagrams start with the Actors or with the (Google it again)? 

Extends, Depends, Inherits and Includes? (Google it again)   

![Use case](https://raw.githubusercontent.com/robertriordan/2400/master/Images/uml_diagram.png)

![Use case](https://raw.githubusercontent.com/robertriordan/2400/master/Images/uml_detail.png)

Context, sequence and class diagrams, too!

![text box](https://raw.githubusercontent.com/robertriordan/2400/master/Images/value_zone.png)




It's in the Telematics/Usage/Data exhaust box where we are finding a lot of value being generated by what were traditional manufacturers or service providers who now find that their data are as or more valuable than their product/service. Witness Uber, Google, GE and Ford (see the industries tab in OneNote.   

The bait must be tasty to the fish, not the fisher. 

Uber might disrupt parcel delivery, pizza delivery (FedEx, UPS, Purolater), prescription delivery, and anything else that's on the road. Uber can sell their on-road experience (real time) data to lost of ppl and could probably make a living just being on the road and f the taxi industry shit. What else can they disrupt? 

The sharing economy: http://sloanreview.mit.edu/article/adapting-to-the-sharing-economy/

<script src='//my.visme.co/visme.js'></script><div class='visme_d' data-url='systems-landscape-a50345' data-w='1366' data-h='768'></div>