##Chapter 2 - Backstory ![Flashlight](https://raw.githubusercontent.com/robertriordan/2400/master/Images/icons/32/time_mach_1.png)

### The five faces of information
Below is an infographic of sorts, showing the five *faces* of information. The faces and their characteristics are from a (highly recommended) 2012 book by *Joel Katz* entitled *Designing Information: Human factor and common sense in design* (2012, John Wiley & Sons).  
<a name="katz"></a>
**Figure FFI. The five faces of information according to Katz**

![Nature of information](https://raw.githubusercontent.com/robertriordan/2400/master/Images/nature_of_info.png)

Here in this chapter, as almost always, many authors use the term *information when they actually mean data* (if you agree with my interpretation and definition). As a rule of thumb, when experts here are talking about exchanging information, in most cases they mean data. A message, for example, such as the spoken word is simply a stream of sounds. It's only information (capable of causing change) when it's interpreted by the recipient and mapped onto concepts that are familiar to her or him. Someone yelling "Fire!" in a language that I don't understand is not information. It's sounds that I can't measure or calibrate or interpret. It's data. Please remain clear on this despite all the loose talk in this chapter.  

I have arranged Katz’s five types into three groups, then ranked them in terms of their importance to us as students of business. Moreover, I have added a column delineating the approach taken by ICT to the challenge of how to deal with each group. Let’s begin at the bottom, and explain each in turn. 

As we walk our way up the scale, consider the context to be that our organisation is examining the results of an online poll asking visitors to a third-party website to rate the various products in our industry (let’s say it’s the toothpaste segment of the dental hygiene industry just to put some teeth in it and we're Sparkl and our competitor is nuGrill). The results have been summarised by the polling firm and released at a press conference sponsored by nuGrill. This could be damaging to us at Sparkl. The opinions of visitors to the website have been measured in the online survey and the raw data have been contextualised by the polling firm and, worst of all, our competitor, nuGrill, has provided its own spin. 

Though the data has been transformed into information in the context of our competitor’s survey and the analysis by the polling firm, for us, it’s still raw data. In order to make a decision whether to act on the release of the data, we need to put it in our own unique context.  

We need to contextualise the input data in order to create information sufficient to decide on an action. Here we go, starting from the bottom of the taxonomy. 

#### Non-information
Non-information is described by Katz as being possibly true (thus perhaps untrue), probably unimportant (perhaps we can ignore it) and/or possibly confusing (not well enough explained or contextualised to allow for understanding). The ICT approach is likely to monitor and filter. If the information is deemed to be of no impact, the ICT response could be to filter it out and remove it from consideration by our company. If deemed potentially relevant, we might conclude that the survey information passes the non-information test, thus we retain it and move up the ladder. Our organisation might have a simple system in place to monitor electronic news services and social media flows looking for keywords (the name *Sparkl Tootpaste* for example, and perhaps negative and positive words in the context). Such monitoring might allow the system to decide whether to move the status of the news item up the scale to alert. News items that fail to meet a minimum criteria would likely be filtered out and not subject to further scrutiny. Thus we are removing potentially *entropic* elements from our data stream. Taking the static off the line. (See under *Disinformation* below for an example of such content monitoring and filtering.)

#### Un-information
Next up is un-information as we move into the orange zone of importance. This is a curious category indeed, but information here is more important (potentially more impactful) than is non-information. It’s slightly up the *fudge scale* in terms of perceived truth, going from only ‘possibly true’ to ‘probably not untrue’ - talk about a fine distinction! It remains probably unimportant, but has moved up the scale from confusing to possibly interesting. We have then, in this situation, some input data that is at least potentially interesting to us at Sparkl. The ICT response might be to alert the firm to the existence and location of the information, and wait for a knowledge worker (who might be you) to decide whether the information warrants escalation or disposal. This is akin to The Toronto Star's shunting potentially dangerous comments to a siding to wait for a real person to take a look at them. 

Assuming the knowledge worker decided on escalation, a further determination is necessary in order to decide on a response. We need to go up another rung on the ladder into the critical red zone. This is where context and information become critical. 

#### Disinformation
The first category in the red zone is ‘disinformation’, which Katz characterised as *deliberately not true* and very likely used tactically to intentionally mislead those consuming it. This information might well have been fabricated by our competitor to cast us in a bad light. Imagine the following news release from nuGrill: "nuGrill commissioned independant research agency TellItAll SurveyWorks to survey an accidental sample of toothpaste lovers, asking them to comment on nuGrill and our competitors. The results show that Sparkl tootpaste causes tooth yellowing and bleeding gums!" This activity might even be illegal (libelous), and could be quite damaging to our reputation. In this context, we will want to take some action to protect ourselves. An ICT response could be to provide data to counter the disinformation and disseminate it to our stakeholders in order to protect ourselves from damage. Such information might be in the form of research indicating the whitening properties of our products. 

Indeed, news organisations are continuously on the lookout for potentially damaging posts in the *Comments* section following many news stories (in fact, many online news publications have abandoned comments altogether owing to the near impossibility of monitoring and moderating them). The *Toronto Star*, for example, wrote the following in an editorial piece appearing on their online site on July 23, 2015, regarding such comment sections:

"Our moderation team spends the bulk of its time working on limiting the worst of the worst: racism, threats and personal attacks. This is done with help from readers who flag comments they deem out of line with our community guidelines and *software that scans for keywords we have identified as sensitive, and moves those comments to a ‘disabled’ queue until an editor can read them.*" [*emphasis* added]

#### Misinformation
If analysis determines that the information released by our competitor nuGrill is not deliberately untrue, it might well be ‘misinformation’, described as definitely not true and important to avoid, in the sense that if the information were considered to be true (even though it is, in fact, *untrue*) any actions taken under the erroneous belief that it *is* true would be inappropriate at least and damaging at worst. This information is toxic and needs to be corrected before inappropriate action is taken by some party believing it to be true. Some party such as Sparkl's customers! The ICT response could be to provide true information to counter the false assertions, which could be automatically provided to our customers through various actions (email, Twitter, Facebook, etc.) made possible by stored electronic customer service records with contact information. 

#### Information
If the information is deemed to be not untrue (*ergo*, it’s true) then we move to the last level in the scale: information. If we make it up to this level, we know that the information released at the press conference is deemed to contain a true representation of our customers’ opinions of Sparkl and, if released by our competitor nuGrill, the results must be saying either something bad about us or something good about them. Whichever, the information is true and action must be taken to deal with the fallout. Several ICT strategies are possible, including targeted information campaigns designed to bolster our reputation and/or discredit the competition. 

#### Exformation
Appropriate in this context is the discussion of the concept of *Exformation*. Here I extensively quote [the exformation entry in wikipedia.com](https://en.wikipedia.org/wiki/Exformation "Exformation") (all emphasis added by me]:

"Exformation (originally spelled *eksformation* in Danish) is a term coined by Danish science writer Tor Nørretranders in his book *The User Illusion* published in English 1998. It is meant to mean *explicitly discarded information*. However, the term also has other meanings related to information, for instance *useful and relevant information* ...

"Effective communication depends on a shared body of knowledge between the persons communicating. In using words, sounds, and gestures, the speaker has deliberately thrown away a huge body of information, though it remains implied. This shared **context** is called *exformation*.

"Exformation is everything we do not actually say but have in our heads when, or before, we say anything at all - whereas information is the measurable, demonstrable utterance we actually come out with. [see Chapter 1 of this textbook.]

If someone is talking about computers, what is said will have more meaning if the person listening has some prior idea what a computer is, what it is good for, and in what *contexts* they might encounter one. From the information content of a message alone, there is no way of measuring how much exformation it contains.

"In 1862 the author Victor Hugo wrote to his publisher asking how his most recent book, *Les Misérables*, was getting on. Hugo just wrote "?" in his message, to which his publisher replied "!", to indicate it was selling well. This exchange of messages would have no meaning to a third party because the shared context is unique to those taking part in it. The amount of information (a single character) was extremely small, and yet because of exformation a meaning is clearly conveyed."

We, with the benefit of knowing the contextual information of who wrote the note and to whom as well as what the inquiry was about (book sales), can piece together the exformation that allows us to understand the communication. We are not always so lucky, and the machines among us are at a decided disadvantage when it comes to exformation.  

**Figure JJLL. Exformation as context**

![Exformation as context](https://raw.githubusercontent.com/robertriordan/2400/master/Images/exformation.png)

Examine Figure JJLL and imagine time-transporting Canada's first PM, Sir John A. Macdonald, to the present day and handing him an Android device (I don't know about you, but I indulge in such fantasies regularly...). Note the massive *exformation* in this interaction - the things that we just implicitly understand about such modern devices that Sir John A. could not possibly fathom. This is context at its most obvious. 

We might think of exformation as *common sense* in a way. Common sense is *context* that is commonly held by individuals in a particular social setting. When someone says "That's just common sense!" it is understood that a person should have internalised the context in which some action was taken and have been guided by it. "It's just common sense that you need a browser to look at a webpage", for example. It's a shared understanding and a product of having been immersed in the computing culture since forever. But what of Sir John A? It would take him quite a while to come up to speed. 

Our question in this section is *“Will machines ever be able to make sense of context and exformation?"* We’ll get there in our discussion. Hang on. 

Meanwhile, take a look at context in practice. Below in Figure GH we see the distribution of scores on the 2013 high school exit examination in Poland. Is human decision making at play here ("We just can't let poor Wojciech fail... he's so close!") or have humans written a computer algorithm that contextualises grades such that "Given that scores have a confidence interval around them of say +/-3%, any score that is in the range of 27-30 should be awarded a grade of 30"? We don't know, but this is interesting *contextual rule-making*, no? Also note the small amount of what's referred to in statistical circles as *heaping* at the highest level (100%). I guess if you get close enough, what's the difference? If you log into *bit.ly* (see below) and examine the data, you will see that there were no scores of 92, 95 or 98, but that the frequency of test scores at 100% was twice that at 99% despite a pattern of continually falling percentages as the distribution approaches 100%. You might also note that no one scored a 29. There was also a bit of a heap at 31 -- so likely a 31 means you scored a legit 30 and were given a 31 so as to differentiate from those who were *gifted* a 30 - that's a message in itself. A 30 means you didn't earn it... And there's a bit of heaping at 32 up to about 36 or so as the *bumping up* continues to ripple through the distribution. An otherwise almost textbook *normal distribution* becomes the victim of context. 

**Figure GH. Context at its finest.**

<div>
    <a href="https://plot.ly/~Dreamshot/461/" target="_blank" title="Distribution of the Results of the Matura in 2013 (Poland&#39;s High School Exit Exam) &lt;br&gt;&lt;br&gt;The minimum score to pass is 30%" style="display: block; text-align: left;"><img src="https://plot.ly/~Dreamshot/461.png" alt="Distribution of the Results of the Matura in 2013 (Poland&#39;s High School Exit Exam)&lt;br&gt;&lt;br&gt;The minimum score to pass is 30%" style="max-width: 100%;width: 700px;"  width="700" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
</div>

*Source: https://plot.ly. Create an account and play around. Fun stuff.*

### Meaning
How do we derive meaning from data or communication? Through interpreting data and facts in context. Context provides meaning for meaningless data. But what does *meaning* mean? Oxford to the rescue:

"Whatever it is that makes what would otherwise be mere sounds [...] into instruments of communication and understanding. The philosophical problem is to demystify this power, and to relate it to what we know of ourselves and the world."

[Interested?](http://bit.ly/1GoQwmb)

What we *know* of ourselves and the world is clearly *knowledge*. But from where comes knowledge?
[Oxford](http://www.oxforddictionaries.com/us/definition/american_english/knowledge "Oxford on knowledge") offers the following (edited for applicability in this context -- see? Context is everywhere).

Knowledge is:

1. **Facts, information**, and skills acquired by a person through **experience** or education; the theoretical or practical understanding of a subject
2. Awareness or familiarity gained by experience of a fact or **situation** 

The important concepts were **bolded** in the definitions above by me. Let’s look at each in turn:

1. *Facts* – facts are data; simple measurements lacking context
2. *Information* – we deal with information below, and how it is derived from data and then becomes actionable through context
3. *Experience* – comes from observation of the results of actions, which flow from decisions fuelled by actions as a result of information produced from data in a context
4. Situation – a synonym for context

### Data architecture

How do we go about ensuring that optimal contexts are built, within which we generate the most valuable information? The answer is *data architecture* and perhaps in the broader field of *information architecture*. Our online Business Dictionary defines *data architecture* as:

>"Models, policies, rules, or standards that govern which data is collected, and how it is stored, arranged, and put to use in a database system, and/or in an organization."

[If you are interested, make sure to follow each link for the related definitions!](http://www.businessdictionary.com/definition/data-architecture.html)

What does all this mean for us? It means that we need to carefully think about *which* data to collect, *how* we will collect it, how we will *store* it and how we will *retrieve* it in order that it will contribute to our decision-making context. We will return to this general topic when we discuss databases and related technology in a subsequent chapter. 

For now, imagine a survey with only one question: "So, what do you think?" Administering this survey to a sample of your cohort would lead to all kinds of rich data, wild imaginings and likely some quite pointed suggestions about what we can do with our *survey*. The data we get in response might well be interesting, but not at all targeted to anything. People would be looking for a context. "What do I think about WHAT?" In fact, I went to the trouble of creating such a survey on a public survey service site but thought better of it... I can imagine the results. It is this principle which guides us in architecting our data collection, storage and retrieval plans for an organisation. We must specify what questions we will need to answer:

- about our organisation when it comes time to account for our activities
- to determine how we are performing
- to understand our place in the market
- to determine how to move forward
- in reply to our employees, customers, stakeholders, regulators, partners and even competitors
  
In this regard, organisations must carefully consider their data strategy. But we must also be aware that simply saving everything in every possible format *just in case* is not a wise strategy either. The burden would be onerous, the data would be monstrous, the information generated from it redundant and perhaps even contradictory. The cost would soon far outweigh the benefits. Rather, an intelligent strategy to specify what data will be required in what volume at what frequency and in which format(s) is the best way forward. 

[Interested?](http://www.itworldcanada.com/article/canada-has-a-dark-data-problem-and-its-getting-worse-veritas/381631)

[McKinsey](http://www.mckinsey.com/insights/high_tech_telecoms_internet/an_executives_guide_to_machine_learning "Machine Learning") had this to say on the subject of whom should be responsible for *marshalling data* in the organisation:

>"Access to troves of useful and reliable data is required for effective machine learning, such as [IBM *Jeopardy!*-winning supercomputing cluster] Watson’s ability, in tests, to predict oncological [cancer] outcomes better than physicians or Facebook’s recent success teaching computers to identify specific human faces nearly as accurately as humans do. A true data strategy starts with identifying gaps in the data, determining the time and money required to fill those gaps, and breaking down silos [see our discussion of the ERP organisation [(take me there)](#erp_org). Too often, departments hoard information and politicize access to it — one reason some companies have created the new role of chief data officer [CDO as opposed to Chief Information Officer (CIO)] to pull together what’s required. Other elements include putting responsibility for generating data in the hands of frontline managers."

### Information Theory 
This all leads to thinking about how to think about information, and for this, we need to dig a bit deeper. Here we go. 

Our business dictionary defines Information Theory as: "Basic data communication theory that applies to the technical processes of encoding a signal for transmission, and provides a statistical description of the message produced by the code. It defines information as choice or entropy and treats the 'meaning' of a message (in the human sense) as irrelevant. Proposed together by the US mathematicians Claude Shannon (1916-2001) and Warren Weaver (1894-1978) in 1949, it focuses on how to transmit data most efficiently and economically, and to detect errors in its transmission and reception."

[Interested?](http://www.businessdictionary.com/definition/information-theory.html)

To really understand Shannon and Weaver (and a guy named Weiner), we need to look a little more deeply into the theory of information.

<a name="entropy"></a>
### Entropy and organisation and *potential* information
From another relatively old (1998) but still [excellent piece](http://www.sveiby.com/articles/Information.html), we find an introduction to the concept of *entropy*. 

>“In the physical sciences the entropy associated with a situation is a measure of the degree of randomness. The second law of thermodynamics states that entropy always increases in the universe. High entropy equals [a] high level of chaos.”  

Thus for decision making, entropy is the enemy. Entropy is *junk on the signal or static on the line.* It thwarts our efforts to make sense of a data transmission and to translate data into information. While entropy and chaos and superfluous data provide richness in terms of the **volume** of signal being sent, they are useless in the context of seeking pointed, surgical, targeted information to answer a specific question and to guide action.

Though this article is too dense to make it an [Interested?] link, it raises some crucial points. Specifically that “The word information is derived from Latin *informare* which means "give form to". [...] Most people tend to think of information as disjointed little bundles of 'facts'. In the Oxford definition of the word it is connected both to knowledge and communication. [...] The way the word information is used can refer to both 'facts' in themselves and the transmission of the facts.”

The author continues. “The double notions of information as both facts and communication are also inherent in one of the foundations of information theory: cybernetics introduced by Norbert Wiener (1948). The cybernetic theory was derived from the new findings in the 1930s and 1940s regarding the role of bioelectric signals in biological systems, including the human being. The full title was: 'Cybernetics or Control and Communication in the Animal and the Machine'. Cybernetics was thus attached to biology from the beginning.

>"Wiener introduces the concepts, amount of information, entropy, feedback and background noise as essential characteristics of how the human brain functions. [...] The notion of the amount of information attaches itself very naturally to a classical notion in statistical mechanics: that of entropy. Just as the amount of information in a system is a measure of its degree of organisation, so the entropy of a system is a measure of its degree of disorganisation."

Thus *entropy* = *disorganisation* = *information*. How odd. How can *dis*organisation yield more *information*? It is because *disorganisation* can produce richer interpretation in that it allows for various different conclusions about the actual message or meaning of the data. But this disorganisation is the enemy of tactical decision making. The process of organising information reduces *dis*organisation in that superfluous elements (not related to a particular context, for example) are removed, yielding more targeted and focused data. 

We can also easily imagine how this notion jives with our contention that systems only add value if they either reduce input or augment output. The more parsimonious we are in architecting our data for specific purposes, the less input will be required in any specific context. Thus thinking about data is important, and planning its collection, storage and retrieval is pivotal to efficiency. Think of this as a geologist would inasmuch as more pure ore makes it easier to produce steel - there is less to discard. Or the more pure and clean a cellular telephone transmission is, the better our understanding of the conversation.  

>"What is information and how is it measured? Wiener defines it as a probability: One of the simplest, most unitary forms of information is the recording of choice between two equally probable simple alternatives, one or the other is bound to happen - a choice, for example, between heads and tails in the tossing of a coin. We shall call a single choice of this sort a decision. If we then ask for the amount of information in the perfectly precise measurement of a quantity known to lie between A and B [...] then the number of choices made and the consequent amount of information is infinite. [...] The quantity that we here define as amount of information is the **negative** of the quantity usually defined as entropy in similar situations." (article author’s bold)

The author continues: "Information is from its conception attached to issues of decisions, communication and control, by Wiener. System theorists build further on this concept and see information as something that is used by a mechanism or organism, a system which is seen as a 'black box', for steering the system towards a predefined goal. The goal is compared with the actual performance and signals are sent back to the sender if the performance deviates from the norm. This concept of negative feedback has proven to be a powerful tool in most control mechanisms, relays etc."

I will now muddy the already turbid water by introducing an opposing viewpoint, that of Claude Shannon, an eminent information scientist working at AT&T (the US telephone people) in the 1950s. The author of the *liquid information* piece writes:

>"The other scientist connected with information theory is Claude Shannon. He was a contemporary of Wiener and as an AT&T mathematician he was primarily interested in the limitations of a channel in transferring signals and the cost of information transfer via a telephone line. He developed a mathematical theory for such communication in *The Mathematical Theory of Communication*, (Shannon & Weaver 1959). Shannon defines information as a purely quantitative measure of communicative exchanges."

So Shannon wasn't interested in *what* was communicated as much as he was in the *occurrence* or *volume* of communication. It's not *what* for him, but *that*. The author continues:

>"[...] based on Shannon it does not matter whether we are communicating a fact, a judgment or just nonsense. Everything we transmit over a telephone line is 'information'. The message 'I feel fine' is information, but 'ff eeI efni' is an equal amount of information."

Note that the message (we cannot say the 'intended message' as we do not know the intention of the sender when sending it) 'I feel fine' is *almost* an anagram of the gibberish 'ff eeI efni' (one extra 'f' and no 'l'), but there may be other possible combinations of letters and spaces that would yield other equally viable messages *in this context*. For Shannon there is information richness in this disorganisation. The potential for many messages means there is more raw information in the message. As we will see, for business, this isn't a good thing. 

In Shannon's defence, the author goes on to write that "Shannon is said to have been unhappy with the word 'information' in his theory. He was advised to use the word 'entropy' instead, but entropy was a concept too difficult to communicate so he remained with the word. Since his theory concerns only transmission of signals, Langefors (1968) suggested that a better term for Shannon’s information theory would therefore perhaps be 'signal transmission theory'."

But we have a problem here. How can one theorist describe information as *organisation* and another describe it as *disorganisation*? The article continues with:

“Weaver, explaining Shannon’s theory in the same book: Information is a measure of one’s freedom of choice in selecting a message. The greater this freedom of choice, the greater the information, the greater is the uncertainty that the message actually selected is some particular one. Greater freedom of choice, greater uncertainty, greater information go hand in hand.”

Here comes the contradiction...

“There is thus one large - and confusing - difference between Shannon and Wiener. Whereas Wiener sees information as negative entropy, i.e. a 'structured piece of the world', Shannon's information is the same as (positive) entropy. This makes Shannon's ‘information’ the opposite of Wiener's ‘information’."

For Shannon, the content of a message (which he calls *information* but which I call *potential* information or simply *data*) is a function of volume. The bigger the message, the greater the information content. Shannon was a telephone company engineer, interested only in *that* data was sent and not *what* data was sent. Shannon did not care *what* people were talking about on the phone but only *that* they were talking. The volume of data transmitted was more important than the actual content. 

And this makes sense if you think about it, from the point of view of a telephone conversation. Imagine you are in a phone call. In the background, you have music playing loudly enough for the other party to hear. While the music is not part of the conversation, *per se*, it becomes an element of the message being sent from you to the other party. It’s background and contributes to the richness of the signal.

But that music does *not* contribute to the clarity of the conversation. Music is competition for understanding the spoken word. 

Thus for Shannon, the more *entropy* (disorder – as in background music), the more disorganisation (lack of focus) and therefore the more *potential* interpretations could be made as a result of the message. If the background music was too loud, for example, the *intended message* might become garbled or unintelligible. This is not helpful for us in business, where we rely on *targeted* almost *surgical* messaging in order to make decisions that result in positive outcomes. This is especially true in information systems, where *interpretation* is not a particularly strong suit of software. Systems are rules-based for the most part. They don't interpret well. Yet. 

Weiner, on the other hand, saw information as *negative entropy*, or positive organisation with structure, interpretability, less equivocation and noise and more certainty. This is the kind of message that business communication requires. Straightforward and to the point. No guessing about the information content of a data stream. *Less input for a given output.*

Entropy is the friend of *data volume* but the enemy of good *decision making*. In business, we need to keep the junk off the signal. Entropy is to be avoided. Structure is valued. Clean communication is the goal. Understanding is critical.

Thus Weiner is our man. We care both *that* messaging is occurring and *what* is being messaged. But Shannon was an inspiring academic. You’ll see reference to him in the section on ASCII. 

[Interested?]( http://www.kerryr.net/pioneers/shannon.htm)

####Takeaways
Business requires careful planning of its data gathering efforts in order to collect clean, precise and intelligible data that covers its business intelligence needs. Data Scientists, Information and Data Architects and others are tasked with designing data-gathering regimes to allow for data to be turned into actionable information that will drive decisions and reduce error around predicted outcomes. Business needs clean, non-noisy data and it's not an easy task to engineer it. 

###Digital vs. Analog
The distinction between the concepts of *analog* and *digital* is important here. The difference between them is akin to the difference between an integer (whole) number and a real number (with decimal precision of varying degrees). Analog is the *real numbers* of nature. Analog is the subtle curve and continuous and apparently seamless change we witness all around us; so subtle that sometimes it’s impossible to tell where one thing ends and another begins. We all know what a leaf is and what a branch is, but at which exact point does a branch become a leaf? This is analog. 

Consider colour. In Figure ECC below, we see colour represented in two different ways, as discrete swatches representing the ROYGBIV colours of the rainbow and then, below that, as a continuum of those visible colours. 

**Figure ECC. Illustrating the difference between analog and digital**

![Analog and digital](https://raw.githubusercontent.com/robertriordan/2400/master/Images/analog_digital.png)

We can (most of us – some people suffer with some form of colour blindness [Interested?]( http://www.colourblindawareness.org/colour-blindness/types-of-colour-blindness/)) easily discern the difference between red and orange, or orange and yellow from among the boxes in the top row of Figure ECC. And we could equally easily point to a green region or a blue region in the continuous strip of colour beneath. The challenge becomes specifying the point at which yellow becomes green, or exactly where indigo becomes violet. Try and pinpoint the exact location where yellow disappears and becomes green as we move left to right. The continuous nature of the colour strip makes it difficult to nail anything down, in fact. 

The colours in the upper box are represented using a specific method of reproducing colour called the RGB method, standing for Red, Green, Blue, two of the three primary colours. All colours, using this method, are produced as a function of mixing more or less of each of these three on a scale from 0 to 255. So the RGB for the colour red is 255, 0, 0. The maximum red (255) and no green or blue. Green is 0, 255, 0 and blue, 0, 0, 255. Of course there are plenty of shades and hues between these values, and indigo and violet off the right end of the spectrum are entities unto themselves. 

It doesn’t matter to us how this or other colour representation methods (such as CMYK, Pantone, etc.) actually work. What matters is that the continuous colour scale represented by the visible spectrum of the rainbow can be *sampled* and *digitised* such that we can work with it in a discrete way. The RGB scale itself produces tints and shades between which the naked eye could not discern. I challenge you to distinguish an RGB of 255, 0, 0 from 254, 0, 0. It would take an expensive display device to even produce and show an image capable of allowing us to discern that difference – but our eyes must be able to work at that level of precision. The message being, at some point or at some resolution, a digital representation becomes just as good as an analog one. It’s just as good because *we can’t tell the difference*. I’m not saying *better* or more *natural* but rather *just as good* for certain purposes. Take a look at Figure JP below. 

**Figure JP. Subtle differences in RGB**

![Can you tell the difference?](https://raw.githubusercontent.com/robertriordan/2400/master/Images/rgb.png)

The four reds are, for certain, red. The normally-sighted would have no trouble identifying each and all as being of the colour red from among the other colours of the visible spectrum. But the differences within the range of red are more difficult to detect. The leftmost (labeled 255) is the same red as in Figure ECC. I have altered the amount of red first from the max to 254, then to 245 and then to 225. The difference between 255 and 254 is so slight that it is nearly impossible to detect at the resolution of that figure on any of my devices or monitors with my eyes (such as they are). *Perhaps a slightly darker tint?* We begin to see a subtle but discernible difference at 245 (a clearly darker tint) while the difference at 225 is quite noticeable. For some purposes, a specific red of a specific hue might be required. For the vast majority of others, any of these reds will do. We would all stop at a traffic light if it showed any of these reds. And that’s the point. In some contexts, *any red* is good enough.

Imagine now, sensing the difference between an RGB value of 255, 0, 0 and 254.5460274, 0, 0. Only in the most exacting and demanding of scientific or engineering contexts would such a difference be important (you see the use of *context* here as a *situation* in which certain things, such as precision, are critical). In 99.999999999999999% of cases where the two were compared, it would make no difference. We can *model* the analog nature of nature and get a *good enough* representation on a digital scale. 

Let's look at this a different way. First of all, try with all your might not to scroll down more than one image at a time below so that the *reveal* can be properly done. Take a look at the image in Figure AGST1 below:

**Figure AGST1. Maximum pixilation**

![Maximum pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_25.png)

Can you decipher anything about it? Other than some basic colours, it's pretty difficult to say what the image represents. Below, the image will increasingly come into focus with each subsequent figure.

**Figure AGST2. Reduced pixilation**

![Level 4 pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_20.png)

Is this one any more helpful? There are some slight clues, but really, there's not enough data here to allow for a definitive choice between a deformed potato or a camel at rest. Which is it?

**Figure AGST3. Reduced pixilation**

![Level 3 pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_15.png)

Here you might be starting to get an idea of what the image represents. A head is perhaps discernable if those black patches are eyes. Could well be an animal, but which animal and in what orientation to the horizon?

**Figure AGST4. Reduced pixilation**

![Level 2 pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_10.png)

Figure AGST4 is likely good enough for most applications. Perhaps if you were the pet owner, you'd want a better resolution in order to be able to decide if it was actually *your* cat or a similarly-sized and -coloured one. But in general, this resolution is *good enough* for most purposes. We don't need, except in very specific contexts, to spend the resources to reproduce an exact image of this kitten. It's already *good enough*. 

**Figure AGST5. Reduced pixilation**

![Level 1 pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_5.png)

Sharper still, this image is almost at a resolution where it's not necessary to go beyond. But we shall.

**Figure AGST6. Original - No pixilation**

![Level 0 pixilation](https://raw.githubusercontent.com/robertriordan/2400/master/Images/cat_0.png)

Figure AGST6 above is the original image. 

*Photo credit: https://sites.psu.edu/siowfa15/2015/10/21/cats-and-their-magic-power/*

This little exercise is pertinent not only to our discussion of just-noticeable difference in this chapter, but also to our treatment of level of measurement in Chapter 1. It will be revisited when we discuss *granularity* in Chapter 3. Thus the notion of *resolution* or *fidelity* or *granularity* is important in the study of information systems. It's a matter of having the right amount of data, measured with the right precision and located in the correct context in order to make decisions. 

I’m hoping that by this point you are starting to see the light. The point being that computers represent everything as a series and a combination of binary digits (bits) and the more bits that can be dedicated to modelling something, the more information can be carried and the finer and finer can be the distinction between discrete elements. So fine, with such massive computing power as we now have, that eventually the binary representation of things becomes so rich, so fluid, that we can no longer tell the difference between the *real thing* and the binary/digital representation of it such that, well, it doesn’t matter at all. Witness Apple’s *retina display* which Apple claims to be so close to analog that our retinae are incapable of discerning anything finer. We can’t tell the difference. And that means it's good enough.

And what self-respecting chapter on this topic would be complete without a reference to the Keanu Reeves / Lawrence Fishburne epic movie series *The Matrix*, first released in 1999, written and directed by The Wachowski Brothers? 

[Wikipedia Interested?]( http://en.wikipedia.org/wiki/The*Matrix) and/or [IMDB Interested?](http://www.imdb.com/title/tt0133093/?ref*=nv*sr*1)

And just to throw a wrench into the works, the notion of human *spirit* and a *soul* play large in this debate. If everything analog can ultimately be represented by a series of bits to the point where we can’t tell the difference, then are people simply hugely complex binary machines, as are computers, made up of such minute and many binary objects that we simply can’t *yet* detect and measure them? Is the analog nature of nature just digital at such a fine level of precision that we simply haven’t seen it yet?  Take a biochemistry course. Take philosophy courses. Everyone should. You aren’t complete without them. The truth is out there ;) 

**Time for an [XKCD](http://xkcd.com/1519/ "XKCD Venus").**

![XKCD Screaming bird of truth](http://imgs.xkcd.com/comics/venus.png)


<a name="sens_beac"></a>
#### Sensors and beacons 

Let's step back and look at two pivotal pieces of technology which we have discussed briefly previously, but need to really *contextualise* in order to fully appreciate their impact. We're talking about *sensors* and their *less-capable* cousins, the *beacons*. Take a look at Figure SB.

**Figure SB. Sensors and beacons**

![How much data is out there?](https://raw.githubusercontent.com/robertriordan/2400/master/Images/sensors_beacons.png)
In making sense of Figure SB, let's begin with the walking icon at the left. The walking motion triggers a *motion detector* (we've all seen these) which might, in this scenario, trigger a door to open (such as at the *LCBO* - where the height of design silliness often causes doors that do not need to be opened to swing abruptlly agape) or a light (in a rarely-used public washroom, for instance) to illuminate. This kind of device can be used to make decisions and cause action in the absence of synchronous decision making by humans. True enough, a person designed the sensor, created the rules about what the sensor should trigger (open a door, etc.) and wrote all the communications protocols and created the servo-mechanisms to effect the door opening, etc. The system was created by humans specifically to make decisions and do work *without our intervention in real time.* Make no mistake, the door is opening as someone approached because humans decided that's what they wanted to happen. But they wanted machines to take care of all the mundane details. This kind of low-level, rules-based automation is everywhere.

But sensor data can also fulfill other ends. Data from sensors can be transmitted not only between other sensors in a network, but also to more central data-collection points and even to the cloud for storage, analysis or transmission to anywhere the *custodians* of the data deem appropriate. So sensor data can contribute enormously to the *context creation* of decision making. All this rich sensor data, growing exponentially, will radically change the machine world's ability to supplant human decision-making in the near term. Get ready for the age of machines precipitated by the so-called *internet of things*. Some call it the *internet of everything*. We will return to the issues around this revolution just below.

[Interested in a local sensor researcher?](http://research.carleton.ca/story/sensor-expert-envisions-future-unobtrusive-monitoring/)

The less sophisticated correlate of the sensor is the *beacon*. While a sensor has the capacity to both read (sense) the environment and share (transmit) data (and fairly rich data), the beacon does no sensing at all. Beacons simply transmit a signal saying "Here I am!" either via their own power (and the issue of how to replenish that power, if required, is interesting) or via power transmitted to it by either a fixed or movable receiver that sends out a message like "Are you there?" Think of the *tap* capability of modern bank ATM and credit cards. Make no mistake. There is no magic in the *tap*. It's simply a way to get the card chip close enough to the reader/transmitter to allow a signal to be received by the card, with just enough power to allow the card to send its data to the reader; your data. There is no power source in the card. It's simply sending everything it knows in response to a query from the reader. Other beacons, however, transmit data either continually or continuously. There are beacons at airports, sending signals to airplanes approaching for landing to give exact coordinates. This is so pilots can safely land their crafts in situations of near-zero visibility. Beacons have been around since antiquity. Strategically-located fires were used by many an army to signal certain situations. Lighthouses are beacons, as are the flashing lights atop emergency vehicles. All send the signal "I am here!" or "Here I am!" 

Specialty applications and hardware/software has been developed by several firms, most notably Apple, with their *iBeacons*. According to Wikipedia (see *Interested?* link below), an industry has sprung up around iBeacons and several vendors have created iBeacon-compatible hardware transmitters, which are a class of *bluetooth* (a widely-used communications protocol) low-energy devices that broadcast their unique identity to nearby portable electronic devices such as smartphones, tablets and other devices, enabling them to perform actions (such as social media *check-ins*) or to receive location-based messages when in close proximity to an iBeacon. A range of activities is possible including launching an app on a mobile smart device when in the vicinity of the beacon. Unlike a generic beacon (which is a one-way transmitter continuously announcing "Here I am!"), the iBeacon requires the installation of a specific app on the receiving device (from the Apple Store or Google Play, for example) thus granting implicit permission for the app to track the owner's whereabouts. Read this paragraph again if you weren't paying attention. Your whereabouts, down to the ketchup section of an aisle in a supermarket, are now a commodity. Our locations are being traded and sold and forwarded and mashed-up by legions of firms worldwide. We can no longer hide. 

[Interested in Bluetooth low energy?](https://en.wikipedia.org/wiki/Bluetooth_low_energy)
[Interested in iBeacons?](https://en.wikipedia.org/wiki/IBeacon)

A rich set of data can be accumulated over time by monitoring beacon movement. If a beacon transmits its location at a particular time and a new location at some time interval later, much data can be generated by calculating the gradients across the time series. We can calculate absolute movement (was in Peterborough at 10:07 but in Ottawa at 14:30), trajectory (path of movement - in this case generally eastward), speed (a rough calculation unless we know the beacon's exact arrival time - it might well have arrived at 13:32 but we didn't get a measurement of its location until 14:30). So plenty of *secondary indicators* can be generated from *primary observations* of a beacon's location. Moreover, when a number of beacons get together at the same time and place, well, all hell can break loose. Imagine the possibilities.

A wealth of data can be collected through embedding beacons in everything from our own bodies to our vehicles (both commercial and personal) and in our aircraft (the famous *black box* is a beacon, allowing searchers to locate a downed airplane if they can find it before box's transmitter runs out of power - and that's a challenge you might want to take if you're at all engineering and entrepreneurial in nature. Imagine the lives you could save by architecting a self-sustaining black box!). And again, all this data can be collected for further analysis, surveillance and prediction. This is true *context.*

[Interested?](https://en.wikipedia.org/wiki/Beacon)

This should help in explaining how ICT can assist in the contextualisation of decision making through providing measured data to contribute to the richness of the solution space. ICT can accomplish this in a number of ways, and not just the traditional method of storing and marshalling legacy data (such as spreadsheet data of last year’s sales, for example), but by providing real-time, synchronous data *representing the current situation* (think sensors and beacons). Things such as identity authentication (authorising you to be in a certain place at a certain time based on either what you have – such as a password or a fingerprint – or what you know, such as a password). Your location in time and space can be known and broadcast. 

And if it can be done for you, it can be done for others. Thus random gatherings of persons in a particular place and time can be sensed and utilised in assembling a context. Even simple things such as GPS and cell tower triangulation can locate you and others. This can facilitate all sorts of crowd-related things such as pop-up retail, policing, and research into facility location. It’s also how things such as geo-fencing are accomplished. Geo-fencing was in the spotlight in 2015 with the use (and abuse) of a little app named *Yik-Yak*. It's also becoming the go-to technology to prevent unauthorised, private drones from encroaching on airport airspace, thereby protecting planes from potential collisions with potentially catastrophic consequences. 

[Interested?]( http://whatis.techtarget.com/definition/geofencing)

In this way, ICT can produce better or more output from the same input as the rich contextualisation of the decision space around *what's happening here?* leads to a more informed (better quality) decision from the same amount of input data.   

ICT can not only use and broadcast your location, it can also sense activity – *what* you’re doing. If your location puts you on the a highway, ICT can make a reasonable guess about what you’re doing, and the error around deciding what it might be can be reduced by examining (measuring) your velocity, for example. Insurance firms are increasingly using voluntary sensors to monitor your driving habits and providing insurance at a lower premium for *good* drivers. 

[Interested?](http://www.ecommercetimes.com/story/75600.html)

Systems can sense and determine lots of activity in which you might be engaged. It can therefore authorise you, based on location, time and activity, to perform certain tasks, such as allow you access to a secure facility or to take possession of rental materials or even rental vehicles. Unattended attendants. How cool. And way fewer resources are required to get those rental materials to you. More output for the same input. *Bingo*.   

Finally (but by no means exhaustively), ICT can contextualise your activities based on your or others’ previous pattern of activity at a time and place. A system might also make some reasonable guesses (though systems don’t guess things, rather they use rules at best and probability at least) about what you are likely to do *next* and anticipate and allow you to discover available services in your vicinity that are appropriate and appealing to you. Walking through the park? Ever rented a canoe to go paddling in the pond? How about a little text message on your phone from Joe’s Canoe Rental? This stuff, called *m-commerce* facilitated by *location services*, is popping up all over. And we’ve just scratched the surface. Mobile payment options now allow commerce between two parties with just cellphones, *disintermediating* financial institutions altogether. 

[Interested?](https://en.wikipedia.org/wiki/Mobile_commerce)

Note that a considerable part of the data that is brought to bear on a context has no direct relationship to the problem or challenge itself, *per se.* The decision space around a firm’s advertising spend this year would contain plenty of direct information, such as the firm’s liquidity position, sales figures, market maturity, cost to advertise in various markets, customer demographics, competitor’s spend, etc. But there are plenty of other, more *contextual* variables that come into play. Things such as the firm’s mission, vision and strategy, ethics, brand value, consumer tastes and trends, resource capabilities and others. All of these things play into the decision but some are less, shall we say, tangible? These things provide context and they are the things at which ICT *has not been as good*, historically. 

But we are on the verge of a change. Context is *the single biggest thing happening in ICT today*. The proliferation of sensors, so tiny and innocuous, is facilitating measurement at such a fine scale and with such sensitivity that soon we’ll have what one author referred to as *liquid information* (see below). Context is so compelling, be sure to grasp the material on Digital vs. Analog above. 

### Liquidity...

Let’s bring this home. A very old but again ever-so-interesting site (www.liquidinformation.org) has [this](http://www.liquidinformation.org/ana_digi_liqui.html) much to offer on the apparently spurious distinction between digital and analog as it pertains to ICT. 

First some definitions:
 
**Analog**: A mechanism in which data is represented by continuously variable physical quantities.

**Digital**: Of or relating to the fingers or toes. Using calculation by numerical methods or by discrete units.

**Liquid**: Flowing freely like water. Having the properties of a liquid: being neither solid nor gaseous. Smooth and unconstrained in movement. 
  
The article offers that:
>“We have been brought up to believe that there is a total distinction, a wall of separation between digital and analog: The world is smooth and continuous; analog whereas computers are operating on discrete, black & white separate units; they are digital. And the twain shall never meet." 

The author continues: 
>“Well you know, it just ain't so. Imagine a couple of small grains of sand. Digital, separate, discrete. Now add a couple more. And a couple more. Millions more. Billions. And you have a beach. An analog, a smooth continuous environment. [sic] 
  
>“Everyday home and office computers, with capacities to manipulate literally billions of bits literally billions of times a second [...] have gone the way of the grains of sand and are definitively not just digital anymore. And they have the potential to become more than analog. They have the potential to become, and make us, liquid.”
 
So the question for us is: At what point does digital become analog? At what threshold does it matter to us? At what point can we detect the difference? What resolution is important for us? This is akin to decimal place precision. How many decimals is it necessary to report in a table of financial ratios, for example, before the additional digit becomes meaningless? Think about this. Especially the Accountants and Finance people among you. How may decimal places, how much precision, is necessary? When is it *good enough*? 

#### Just-noticeable differences and your brain on music
This introduces the notion of *Just-noticeable Difference* or JND. And of course a just-noticeable difference is context dependent (isn’t everything?). The message, again, is that at some point in the digitisation of analog phenomena, the distinction disappears and we can’t tell the difference. JND has applications in many areas, not the least of which is marketing, where intensive research has been done into how much a product can shrink, for example, before potential buyers notice a change in package size.  

[Interested in Marketing?](http://www.scoop.it/t/psychology-of-consumer-behaviour/?tag=Just+Noticeable+Difference)

[Interested in Weber's Law of JND?](http://apps.usd.edu/coglab/WebersLaw.html)

[Interested in the Weber-Fechner Law of JND?](http://en.wikipedia.org/wiki/Just-noticeable_difference)

I’m reading a few books as I am writing this text. One that is particularly interesting to me is entitled *This is Your Brain on Music: The Science of a Human Obsession* by McGill neuroscientist and musician Daniel Levitin (2006, Penguin). He writes that “Less well known are the extraordinary advances we have been able to make in modeling how our neurons work, thanks to the continuing revolution in computer technology. We are coming to understand computational systems in our head like never before.” 

He continues that “Even consciousness itself is no longer shrouded in a mystical fog, but rather is something that emerges from observable physical systems.”

But what’s more interesting for us is the parallel he draws between sound and sight. Consider a motion picture. In case you didn’t know, a movie is made up of a series of still photos interspersed with black screens. When shown at the right speed, however, our visual system can’t tell that it’s a series of stills. Here’s what Levitin writes: 
>“The lowest note on a standard piano vibrates with a frequency of 27.5 Hz. Interestingly, this is about the same rate of motion that constitutes an important threshold in visual perception. [...] ‘Motion pictures’ are a sequence of still images alternating with pieces of black film presented at a rate (one forty-eighth of a second) that exceeds the temporal receiving property of the human visual system. We perceive smooth, continuous motion when in fact there is no such thing actually being shown to us.”  

So motion pictures are *just as good* as the real thing for us. Next time you're out on the road, take a look at the wheels of vehicles as they pass on the street. Note that for many, it appears as if you can see the disk brakes behind the wheels as if there were almost no spokes in the rims at all. Our eyes can't keep up with the spinning speed and our brain can't paint the picture fast enough for us, rather just showing us that there's nothing there. But we know different. 

Finally, just for fun...

**Figure WWE. The *wagon wheel effect*** 

![ICT context](https://raw.githubusercontent.com/robertriordan/2400/master/Images/WagonWheelEffect.gif)

The *Wagon wheel effect* is often seen in old Western movies where the wheels of a horse-drawn carriage sometimes appear to either be frozen or to be rotating counter to the direction in which the wagon is moving. The animated GIF in Figure WWE illustrates the phenomenon. The speed of the *camera*, moving towards the right, constantly increases at the same rate with the objects sliding to the left. Halfway through the 24-second loop, the objects appear to suddenly shift and head backwards.

*Credit: "WagonWheelEffect" by Ulillillia at the English language Wikipedia. Licensed under CC BY-SA 3.0 via Wikimedia Commons - https://commons.wikimedia.org/wiki/File:WagonWheelEffect.gif#/media/File:WagonWheelEffect.gif*

[Interested?](https://en.wikipedia.org/wiki/Wagon-wheel_effect)

We might also want to consider the concept of *resolution*. The resolution of a sensor is the smallest change it can detect in the quantity that it is measuring. Sensor resolution is being continuously improved.

[Interested?](http://en.wikipedia.org/wiki/Sensor)

Why matters all of this? It matters because as systems and sensors proliferate and their ability to communicate improves, so improves their ability to measure with more and more precision in more and more places at lower and lower resolutions at increasing rates of speed providing a richer and richer stream of input data to flow into and to generate our contexts. And that’s how decisions get made. As ICT is increasingly able to provide more and more context, machines are more and more able to work at a resolution level that is not only *good enough* for most situations, but maybe better than we humans can do... And with such great context, better and better information is generated and more and more decisions can be made for us by machines far below our level of consciousness. We'll be the spinning wheels. So fast, we can't see it happen. 

### And finally, the rapidly declining cost of computing

We end this chapter with an astonishing table from our friends at Wikipedia on the cost of computing. A *gigaflop* is described as *the power to execute one billion floating point calculations per second*. A floating point operation (or a FLOP) is any algebraic manipulation (+ - * /) involving two numbers which have decimal places. So 2 (two) is an *integer* but 2.0 (two point zero) is a *floating point number*. *Ergo* a FLOP might be the mathematical operation to multiply 1.0 by 1.0. Floating point arithmetic takes longer than integer arithmetic. A computer processor's power is often measured in *gigaflops*, providing a universal standard for speed of execution (the *s* at the end stands for *per second*), which is a proxy for computing power. The more flops a system can do in a second, the more powerful the system.

Below is a table equating gigaflops with dollars. How much did the computing power to effect one gigaflop cost, down through the annals of computer history? Here's what Wikipedia has to say (it will astonish you):

**Table GFP. Hardware cost to get a gig going**

| **Date** | **Approximate cost per GFLOPS** | **Approximate cost per GFLOPS** [adjusted to 2013 USD] | **Platform providing the lowest cost per GFLOPS** | **Comments** |
| :-: | :-: | :-: | :-: | :- |
| 1961 | US $1,100,000,000,000 ($1.1 trillion) | US $8.3 trillion | About 17 million IBM 1620 units costing $64,000 each | The 1620's multiplication operation takes 17.7 ms. |
| 1984 |$18,750,000 | $42,780,000 |Cray X-MP/48 | $15,000,000 / 0.8 GFLOPS |
| 1997 | $30,000 | $42,000 | Two 16-processor Beowulf clusters with Pentium Pro microprocessors |---|
| April 2000 | $1,000 | $1,300 | Bunyip Beowulf cluster | Bunyip was the first sub-US-$1/MFLOPS computing technology. It won the Gordon Bell Prize in 2000.|
| May 2000 | $640 | $836 | KLAT2 |KLAT2 was the first computing technology which scaled to large applications while staying under US-$1/MFLOPS. |
| August 2003 | $82 | $100 | KASY0 |KASY0 was the first sub-US-$100/GFLOPS computing technology. |
| August 2007 | $48 | $52 | Microwulf | As of August 2007, this 26.25 GFLOPS "personal" Beowulf cluster can be built for $1256. |
| March 2011 | $1.80 | $1.80 | HPU4Science | This $30,000 cluster was built using only commercially available "gamer" grade hardware. |
| June 2013 | $0.22 | $0.22 | Sony Playstation 4 | The Sony PlayStation 4 is listed as having a peak performance of 1.84 TFLOPS, at a price of $400 |
| November 2013 | $0.16 | $0.16 | AMD Sempron 145 GeForce GTX 760 System | Built using commercially available parts, a system using one AMD Sempron 145 and three GeForce GTX 760 reaches a total of 6.771 TFLOPS for a total cost of $1090.66. |
| December 2013 | $0.12 | $0.12 | Pentium G550 R9 290 System | Built using commercially available parts. Pentium G550 & AMD R9 290 tops out at 4.848 TFLOPS grand total of $681.84 USD. |
| January 2015 | $0.08 | $0.08 | Celeron G1830 R9 295x2 System | Built using commercially available parts. Intel Celeron G1830 & AMD Radeon R9 295x2 tops out at over 11.5 TFLOPS at a grand total of $902.57 USD. |

See [the source](http://en.wikipedia.org/wiki/FLOPS) for important notes and additional information.

In 55 years (shorter than my already short life) the equivalent cost to execute a billion FLOPs has dropped from USD $8,300,000,000,000 to USD $0.08. If that's not stunning and breathtaking then neither is (insert your most stunning and breathtaking thing here). Computing wins. 

While a bit dated, in a liquidinformation.org piece representing probably the absolute minimum achievable computing power, it is asserted that, for USD $1,000, we will be able to buy a computer with the computing power of an insect in 2018. That same $1K will get us a mouse brain in 2030, a human brain is 2042 and *all human brains combined* by 2060. If you are a normal early 20s undergraduate student, 2060 is well within your lifetime. Think of it. Just *think.*  

[Interested?](http://www.liquidinformation.org/information_history.html) {read just the *Computation* sub-section including the *When will it end?* piece]

