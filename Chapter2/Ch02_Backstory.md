### Situatedness
An interesting corollary of context is the notion of *situatedness*. Oxford Reference defines it partly as:

“The dependence of meaning (and/or identity) on the specifics of particular sociohistorical, geographical, and cultural contexts, social and power relations, and philosophical and ideological frameworks, within which the multiple perspectives of social actors are *dynamically constructed, negotiated, and contested.*” [emphasis added] 

[Interested?](http://bit.ly/1svpjLY)

Ever answered to someone “You had to be there.” in reply to the question “What’s so funny?” If so, then you appreciate *context*. The interpretation of a message or communication is dependent on the situation in which it occurs and to which it refers. Imagine how many ways a simple word like *Yes* or *No* could be interpreted when you consider situational variables such as voice inflection, facial expression, volume and the length of utterance of the speaker, emphasis on a particular word or even syllable, to name but a few. There’s an hilarious comedy skit by a Jewish comic in which he successively puts the emphasis on each word in the interrogative phrase “He said I should bring a gift?” and in so doing changes the interpretation of the phrase each time. Try it yourself: Emphasise the **bolded** word in each sentence below by making the word a question in and of itself and note how the meaning attached to the phrase changes each time:

**He?** said I should bring a gift?

He **said?** I should bring a gift?

He said **I?** should bring a gift?

He said I **should?** bring a gift?

And so forth. 

There are literally dozens of possible interpretations or meanings for a simple word, depending on the context in which the word was uttered. Misreading the context of a situation can be a very serious matter. People are in jail for it. Lives have been ruined by it. If someone has ever quoted you *out of context* then you know what we’re talking about here. So subtle and so powerful is context that we must dynamically manage all our behaviours in order to be appropriate *in context*.

Oxford suggests that context is dynamically constructed, negotiated and contested. I would add that people navigate and interpret context all the time. We are social actors who are continuously jostling for a position of advantage among our peers and contemporaries. Such is the human condition.

Consider, if you will, the enormous scope of what humans do in constructing context. Look at all the terms in Figure LJST (above) and how each nuanced difference in term conjures up a slightly different meaning for we humans. Milieu, surroundings, circumstance, setting, background, scope, ecology. arena, domain, state of affairs and indeed, *context.*   

#### Exformation
Appropriate in this context is the discussion of the concept of *Exformation*. Here I extensively quote [the exformation entry in wikipedia.com](https://en.wikipedia.org/wiki/Exformation "Exformation") (all emphasis added by me]:

"Exformation (originally spelled *eksformation* in Danish) is a term coined by Danish science writer Tor Nørretranders in his book *The User Illusion* published in English 1998. It is meant to mean *explicitly discarded information*. However, the term also has other meanings related to information, for instance *useful and relevant information* ...

"Effective communication depends on a shared body of knowledge between the persons communicating. In using words, sounds, and gestures, the speaker has deliberately thrown away a huge body of information, though it remains implied. This shared **context** is called *exformation*.

"Exformation is everything we do not actually say but have in our heads when, or before, we say anything at all - whereas information is the measurable, demonstrable utterance we actually come out with. [see Chapter 1 of this textbook.]

If someone is talking about computers, what is said will have more meaning if the person listening has some prior idea what a computer is, what it is good for, and in what *contexts* they might encounter one. From the information content of a message alone, there is no way of measuring how much exformation it contains.

"In 1862 the author Victor Hugo wrote to his publisher asking how his most recent book, *Les Misérables*, was getting on. Hugo just wrote "?" in his message, to which his publisher replied "!", to indicate it was selling well. This exchange of messages would have no meaning to a third party because the shared context is unique to those taking part in it. The amount of information (a single character) was extremely small, and yet because of exformation a meaning is clearly conveyed."

We, with the benefit of knowing the contextual information of who wrote the note and to whom as well as what the inquiry was about (book sales), can piece together the exformation that allows us to understand the communication. We are not always so lucky, and the machines among us are at a decided disadvantage when it comes to exformation.  

**Figure JJLL. Exformation as context**

![Exformation as context](https://raw.githubusercontent.com/robertriordan/2400/master/Images/exformation.png)

Examine Figure JJLL and imagine time-transporting Canada's first PM, Sir John A. Macdonald, to the present day and handing him an Android device (I don't know about you, but I indulge in such fantasies regularly...). Note the massive *exformation* in this interaction - the things that we just implicitly understand about such modern devices that Sir John A. could not possibly fathom. This is context at its most obvious. 

We might think of exformation as *common sense* in a way. Common sense is *context* that is commonly held by individuals in a particular social setting. When someone says "That's just common sense!" it is understood that a person should have internalised the context in which some action was taken and have been guided by it. "It's just common sense that you need a browser to look at a webpage", for example. It's a shared understanding and a product of having been immersed in the computing culture since forever. But what of Sir John A? It would take him quite a while to come up to speed. 

Our question in this section is *“Will machines ever be able to make sense of context and exformation?"* We’ll get there in our discussion. Hang on. 

Meanwhile, take a look at context in practice. Below in Figure GH we see the distribution of scores on the 2013 high school exit examination in Poland. Is human decision making at play here ("We just can't let poor Wojciech fail... he's so close!") or have humans written a computer algorithm that contextualises grades such that "Given that scores have a confidence interval around them of say +/-3%, any score that is in the range of 27-30 should be awarded a grade of 30"? We don't know, but this is interesting *contextual rule-making*, no? Also note the small amount of what's referred to in statistical circles as *heaping* at the highest level (100%). I guess if you get close enough, what's the difference? If you log into *bit.ly* (see below) and examine the data, you will see that there were no scores of 92, 95 or 98, but that the frequency of test scores at 100% was twice that at 99% despite a pattern of continually falling percentages as the distribution approaches 100%. You might also note that no one scored a 29. There was also a bit of a heap at 31 -- so likely a 31 means you scored a legit 30 and were given a 31 so as to differentiate from those who were *gifted* a 30 - that's a message in itself. A 30 means you didn't earn it... And there's a bit of heaping at 32 up to about 36 or so as the *bumping up* continues to ripple through the distribution. An otherwise almost textbook *normal distribution* becomes the victim of context. 

**Figure GH. Context at its finest.**

<div>
    <a href="https://plot.ly/~Dreamshot/461/" target="_blank" title="Distribution of the Results of the Matura in 2013 (Poland&#39;s High School Exit Exam) &lt;br&gt;&lt;br&gt;The minimum score to pass is 30%" style="display: block; text-align: left;"><img src="https://plot.ly/~Dreamshot/461.png" alt="Distribution of the Results of the Matura in 2013 (Poland&#39;s High School Exit Exam)&lt;br&gt;&lt;br&gt;The minimum score to pass is 30%" style="max-width: 100%;width: 700px;"  width="700" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
</div>

*Source: https://plot.ly. Create an account and play around. Fun stuff.*

### Meaning
How do we derive meaning from data or communication? Through interpreting data and facts in context. Context provides meaning for meaningless data. But what does *meaning* mean? Oxford to the rescue:

"Whatever it is that makes what would otherwise be mere sounds [...] into instruments of communication and understanding. The philosophical problem is to demystify this power, and to relate it to what we know of ourselves and the world."

[Interested?](http://bit.ly/1GoQwmb)

What we *know* of ourselves and the world is clearly *knowledge*. But from where comes knowledge?
[Oxford](http://www.oxforddictionaries.com/us/definition/american_english/knowledge "Oxford on knowledge") offers the following (edited for applicability in this context -- see? Context is everywhere).

Knowledge is:

1. **Facts, information**, and skills acquired by a person through **experience** or education; the theoretical or practical understanding of a subject
2. Awareness or familiarity gained by experience of a fact or **situation** 

The important concepts were **bolded** in the definitions above by me. Let’s look at each in turn:

1. *Facts* – facts are data; simple measurements lacking context
2. *Information* – we deal with information below, and how it is derived from data and then becomes actionable through context
3. *Experience* – comes from observation of the results of actions, which flow from decisions fuelled by actions as a result of information produced from data in a context
4. Situation – a synonym for context

### Data architecture

How do we go about ensuring that optimal contexts are built, within which we generate the most valuable information? The answer is *data architecture* and perhaps in the broader field of *information architecture*. Our online Business Dictionary defines *data architecture* as:

"Models, policies, rules, or standards that govern which data is collected, and how it is stored, arranged, and put to use in a database system, and/or in an organization."

[If you are interested, make sure to follow each link for the related definitions!](http://www.businessdictionary.com/definition/data-architecture.html)

What does all this mean for us? It means that we need to carefully think about *which* data to collect, *how* we will collect it, how we will *store* it and how we will *retrieve* it in order that it will contribute to our decision-making context. We will return to this general topic when we discuss databases and related technology in a subsequent chapter. 

For now, imagine a survey with only one question: "So, what do you think?" Administering this survey to a sample of your cohort would lead to all kinds of rich data, wild imaginings and likely some quite pointed suggestions about what we can do with our *survey*. The data we get in response might well be interesting, but not at all targeted to anything. People would be looking for a context. "What do I think about WHAT?" In fact, I went to the trouble of creating such a survey on a public survey service site but thought better of it... I can imagine the results. It is this principle which guides us in architecting our data collection, storage and retrieval plans for an organisation. We must specify what questions we will need to answer:

- about our organisation when it comes time to account for our activities
- to determine how we are performing
- to understand our place in the market
- to determine how to move forward
- in reply to our employees, customers, stakeholders, regulators, partners and even competitors
  
In this regard, organisations must carefully consider their data strategy. But we must also be aware that simply saving everything in every possible format *just in case* is not a wise strategy either. The burden would be onerous, the data would be monstrous, the information generated from it redundant and perhaps even contradictory. The cost would soon far outweigh the benefits. Rather, an intelligent strategy to specify what data will be required in what volume at what frequency and in which format(s) is the best way forward. 

[Interested?](http://www.itworldcanada.com/article/canada-has-a-dark-data-problem-and-its-getting-worse-veritas/381631)

[McKinsey](http://www.mckinsey.com/insights/high_tech_telecoms_internet/an_executives_guide_to_machine_learning "Machine Learning") had this to say on the subject of whom should be responsible for *marshalling data* in the organisation:

"Access to troves of useful and reliable data is required for effective machine learning, such as [IBM *Jeopardy!*-winning supercomputing cluster] Watson’s ability, in tests, to predict oncological [cancer] outcomes better than physicians or Facebook’s recent success teaching computers to identify specific human faces nearly as accurately as humans do. A true data strategy starts with identifying gaps in the data, determining the time and money required to fill those gaps, and breaking down silos [see our discussion of the ERP organisation [(take me there)](#erp_org). Too often, departments hoard information and politicize access to it—one reason some companies have created the new role of chief data officer [CDO as opposed to Chief Information Officer (CIO)] to pull together what’s required. Other elements include putting responsibility for generating data in the hands of frontline managers."

### Information Theory 
This all leads to thinking about how to think about information, and for this, we need to dig a bit deeper. Here we go. 

Our business dictionary defines Information Theory as: "Basic data communication theory that applies to the technical processes of encoding a signal for transmission, and provides a statistical description of the message produced by the code. It defines information as choice or entropy and treats the 'meaning' of a message (in the human sense) as irrelevant. Proposed together by the US mathematicians Claude Shannon (1916-2001) and Warren Weaver (1894-1978) in 1949, it focuses on how to transmit data most efficiently and economically, and to detect errors in its transmission and reception."

[Interested?](http://www.businessdictionary.com/definition/information-theory.html)

To really understand Shannon and Weaver (and a guy named Weiner), we need to look a little more deeply into the theory of information.

<a name="entropy"></a>
### Entropy and organisation and *potential* information
From another relatively old (1998) but still [excellent piece](http://www.sveiby.com/articles/Information.html), we find an introduction to the concept of *entropy*. 

“In the physical sciences the entropy associated with a situation is a measure of the degree of randomness. The second law of thermodynamics states that entropy always increases in the universe. High entropy equals [a] high level of chaos.”  

Thus for decision making, entropy is the enemy. Entropy is *junk on the signal or static on the line.* It thwarts our efforts to make sense of a data transmission and to translate data into information. While entropy and chaos and superfluous data provide richness in terms of the **volume** of signal being sent, they are useless in the context of seeking pointed, surgical, targeted information to answer a specific question and to guide action.

Though this article is too dense to make it an [Interested?] link, it raises some crucial points. Specifically that “The word information is derived from Latin *informare* which means "give form to". [...] Most people tend to think of information as disjointed little bundles of 'facts'. In the Oxford definition of the word it is connected both to knowledge and communication. [...] The way the word information is used can refer to both 'facts' in themselves and the transmission of the facts.”

The author continues. “The double notions of information as both facts and communication are also inherent in one of the foundations of information theory: cybernetics introduced by Norbert Wiener (1948). The cybernetic theory was derived from the new findings in the 1930s and 1940s regarding the role of bioelectric signals in biological systems, including the human being. The full title was: 'Cybernetics or Control and Communication in the Animal and the Machine'. Cybernetics was thus attached to biology from the beginning.

"Wiener introduces the concepts, amount of information, entropy, feedback and background noise as essential characteristics of how the human brain functions. [...] "The notion of the amount of information attaches itself very naturally to a classical notion in statistical mechanics: that of entropy. Just as the amount of information in a system is a measure of its degree of organisation, so the entropy of a system is a measure of its degree of disorganisation. [...]

Thus *entropy* = *disorganisation* = *information*. How odd. How can *dis*organisation yield more *information*? It is because *disorganisation* can produce richer interpretation in that it allows for various different conclusions about the actual message or meaning of the data. But this disorganisation is the enemy of tactical decision making. The process of organising information reduces *dis*organisation in that superfluous elements (not related to a particular context, for example) are removed, yielding more targeted and focused data. 

We can also easily imagine how this notion jives with our contention that systems only add value if they either reduce input or augment output. The more parsimonious we are in architecting our data for specific purposes, the less input will be required in any specific context. Thus thinking about data is important, and planning its collection, storage and retrieval is pivotal to efficiency. Think of this as a geologist would inasmuch as more pure ore makes it easier to produce steel - there is less to discard. Or the more pure and clean a cell transmission is, the better our understanding of the conversation.  

"What is information and how is it measured? Wiener defines it as a probability: One of the simplest, most unitary forms of information is the recording of choice between two equally probable simple alternatives, one or the other is bound to happen - a choice, for example, between heads and tails in the tossing of a coin. We shall call a single choice of this sort a decision. If we then ask for the amount of information in the perfectly precise measurement of a quantity known to lie between A and B [...] then the number of choices made and the consequent amount of information is infinite. [...] The quantity that we here define as amount of information is the **negative** of the quantity usually defined as entropy in similar situations." (article author’s bold)

The author continues: "Information is from its conception attached to issues of decisions, communication and control, by Wiener. System theorists build further on this concept and see information as something that is used by a mechanism or organism, a system which is seen as a 'black box', for steering the system towards a predefined goal. The goal is compared with the actual performance and signals are sent back to the sender if the performance deviates from the norm. This concept of negative feedback has proven to be a powerful tool in most control mechanisms, relays etc."

I will now muddy the already turbid water by introducing an opposing viewpoint, that of Claude Shannon, an eminent information scientist working at AT&T (the US telephone people) in the 1950s. The author writes:

"The other scientist connected with information theory is Claude Shannon. He was a contemporary of Wiener and as an AT&T mathematician he was primarily interested in the limitations of a channel in transferring signals and the cost of information transfer via a telephone line. He developed a mathematical theory for such communication in *The Mathematical Theory of Communication*, (Shannon & Weaver 1959). Shannon defines information as a purely quantitative measure of communicative exchanges."

So Shannon wasn't interested in *what* was communicated as much as he was in the *occurrence* or *volume* of communication. It's not *what* for him, but *that*. The author continues:

"[...] based on Shannon it does not matter whether we are communicating a fact, a judgment or just nonsense. Everything we transmit over a telephone line is 'information'. The message 'I feel fine' is information, but 'ff eeI efni' is an equal amount of information."

Note that the message (we cannot say the 'intended message' as we do not know the intention of the sender when sending it) 'I feel fine' is *almost* an anagram of the gibberish 'ff eeI efni' (one extra 'f' and no 'l'), but there may be other possible combinations of letters and spaces that would yield other equally viable messages *in this context*. For Shannon there is information richness in this disorganisation. The potential for many messages means there is more raw information in the message. As we will see, for business, this isn't a good thing. 

In Shannon's defence, the author goes on to write that "Shannon is said to have been unhappy with the word 'information' in his theory. He was advised to use the word 'entropy' instead, but entropy was a concept too difficult to communicate so he remained with the word. Since his theory concerns only transmission of signals, Langefors (1968) suggested that a better term for Shannon’s information theory would therefore perhaps be 'signal transmission theory'."

But we have a problem here. How can one theorist describe information as *organisation* and another describe it as *disorganisation*? The article continues with:

“Weaver, explaining Shannon’s theory in the same book: Information is a measure of one’s freedom of choice in selecting a message. The greater this freedom of choice, the greater the information, the greater is the uncertainty that the message actually selected is some particular one. Greater freedom of choice, greater uncertainty, greater information go hand in hand.”

Here comes the contradiction...

“There is thus one large - and confusing - difference between Shannon and Wiener. Whereas Wiener sees information as negative entropy, i.e. a 'structured piece of the world', Shannon's information is the same as (positive) entropy. This makes Shannon's ‘information’ the opposite of Wiener's ‘information’."

For Shannon, the content of a message (which he calls *information* but which I call *potential* information or simply *data*) is a function of volume. The bigger the message, the greater the information content. Shannon was a telephone company engineer, interested only in *that* data was sent and not *what* data was sent. Shannon did not care *what* people were talking about on the phone but only *that* they were talking. The volume of data transmitted was more important than the actual content. 

And this makes sense if you think about it, from the point of view of a telephone conversation. Imagine you are in a phone call. In the background, you have music playing loudly enough for the other party to hear. While the music is not part of the conversation, *per se*, it becomes an element of the message being sent from you to the other party. It’s background and contributes to the richness of the signal.

But that music does *not* contribute to the clarity of the conversation. Music is competition for understanding the spoken word. 

Thus for Shannon, the more *entropy* (disorder – as in background music), the more disorganisation (lack of focus) and therefore the more *potential* interpretations could be made as a result of the message. If the background music was too loud, for example, the *intended message* might become garbled or unintelligible. This is not helpful for us in business, where we rely on *targeted* almost *surgical* messaging in order to make decisions that result in positive outcomes. This is especially true in information systems, where *interpretation* is not a particularly strong suit of software. Systems are rule based for the most part. They don't interpret well. Yet. 

Weiner, on the other hand, saw information as *negative entropy*, or positive organisation with structure, interpretability, less equivocation and noise and more certainty. This is the kind of message that business communication requires. Straightforward and to the point. No guessing about the information content of a data stream. *Less input for a given output.*

Entropy is the friend of *data volume* but the enemy of good *decision making*. In business, we need to keep the junk off the signal. Entropy is to be avoided. Structure is valued. Clean communication is the goal. Understanding is critical.

Thus Weiner is our man. We care both *that* messaging is occurring and *what* is being messaged. But Shannon was an inspiring academic. You’ll see reference to him in the section on ASCII. 

[Interested?]( http://www.kerryr.net/pioneers/shannon.htm)

####Takeaways
Business requires careful planning of its data gathering efforts in order to collect clean, precise and intelligible data that covers its business intelligence needs. Data Scientists, Information and Data Architects and others are tasked with desiging data-gathering regimes to allow for data to be turned into actionalbe information that will drive decisions and reduce error around predicted outcomes. Business needs clean, non-noisy data and it's not an easy task to engineer it. 